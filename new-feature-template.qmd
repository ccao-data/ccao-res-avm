---
title: "New Feature Template"
subtitle: "Run ID: `r params$added_variable`"
date: "`r Sys.Date()`"
author: "Cook County Assessor's Office Data Department"
execute:
  echo: false
  warning: false
format:
  html:
    embed-resources: true
    toc: true
    toc_float: true
    fig-align: center
    fontsize: 12pt
params:
  run_id: "2024-03-17-stupefied-maya"
  run_id_year: "2024"
  comparison_run_id: "2024-06-18-calm-nathan"
  comparison_run_id_year: "2024"
  added_variable: "prox_num_bus_stop_in_half_mile"
  added_variable_shap: "prox_num_bus_stop_in_half_mile_shap"
  range: "20 - 25"
  type: "continuous"
---

```{r, echo = FALSE}
library(purrr)
library(here)
library(leaflet)
library(sf)
library(DT)
library(ggplot2)
```

```{r setup, echo = FALSE}
# Load list of helper files and main libraries
purrr::walk(list.files(here::here("R"), "\\.R$", full.names = TRUE), source)

# Initialize a dictionary of file paths. See misc/file_dict.csv for details
paths <- model_file_dict(params$run_id, params$year)

# Load reporting-only R libraries
suppressPackageStartupMessages({
  reporting_libs <- "Config/renv/profiles/reporting/dependencies"
  purrr::walk(
    strsplit(read_yaml(here::here("DESCRIPTION"))[[reporting_libs]], ", ")[[1]],
    library,
    character.only = TRUE
  )
})
```

```{r download_new_data, echo = FALSE}
analyses_paths <- list(
  output = list(
    list(
      s3 = paste0("s3://ccao-model-results-us-east-1/performance/year=", params$run_id_year, "/stage=assessment/", params$run_id, ".parquet"),
      local = file.path("analyses", "new_run", paste0(params$run_id, "-performance.parquet"))
    ),
    list(
      s3 = paste0("s3://ccao-model-results-us-east-1/metadata/year=", params$run_id_year, "/", params$run_id, ".parquet"),
      local = file.path("analyses", "new_run", paste0(params$run_id, "-metadata.parquet"))
    ),
    list(
      s3 = paste0("s3://ccao-model-results-us-east-1/shap/year=", params$run_id_year, "/run_id=", params$run_id, "/"),
      local = file.path("analyses", "new_run", paste0(params$run_id, "-shap.parquet"))
    ),
    list(
      s3 = paste0("s3://ccao-model-results-us-east-1/assessment_card/year=", params$run_id_year, "/run_id=", params$run_id, "/"),
      local = file.path("analyses", "new_run", paste0(params$run_id, "-assessment_card.parquet"))
    ),
    list(
      s3 = paste0("s3://ccao-model-results-us-east-1/assessment_pin/year=", params$run_id_year, "/run_id=", params$run_id, "/"),
      local = file.path("analyses", "new_run", paste0(params$run_id, "-assessment_pin.parquet"))
  )
))
fetch_analyses <- function(run_id, year, analyses_paths) {
  tictoc::tic(paste0("Fetched run: ", run_id))

  s3_objs <- grep("s3://", unlist(analyses_paths$output), value = TRUE)
  bucket <- strsplit(s3_objs[1], "/")[[1]][3]

  for (analyses_path in analyses_paths$output) {
    is_directory <- endsWith(analyses_path$s3, "/")
    if (is_directory) {
      partitioned_by_run <- endsWith(analyses_path$s3, paste0("run_id=", run_id, "/"))
      if (partitioned_by_run) {
        dir_path <- analyses_path$s3
      } else {
        dir_path <- paste0(analyses_path$s3, "year=", year, "/run_id=", run_id, "/")
      }

      message("Now fetching: ", dir_path)
      objs_prefix <- sub(paste0("s3://", bucket, "/"), "", dir_path)
      objs <- aws.s3::get_bucket_df(bucket, objs_prefix)
      objs <- dplyr::filter(objs, Size > 0)
      
      if (nrow(objs) > 0) {
        combined_data <- NULL
        for (key in objs$Key) {
          message("Now fetching: ", key)
          local_temp_path <- file.path(tempdir(), basename(key))
          aws.s3::save_object(key, bucket = bucket, file = local_temp_path)
          
          # Read the Parquet file and append it to combined_data
          temp_data <- arrow::read_parquet(local_temp_path)
          if (is.null(combined_data)) {
            combined_data <- temp_data
          } else {
            combined_data <- dplyr::bind_rows(combined_data, temp_data)
          }
        }
        
        # Save the combined Parquet file
        arrow::write_parquet(combined_data, analyses_path$local)
      } else {
        warning(analyses_path$local, " does not exist for this run")
      }
    } else {
      message("Now fetching: ", analyses_path$s3)
      if (aws.s3::object_exists(analyses_path$s3, bucket = bucket)) {
        aws.s3::save_object(analyses_path$s3, bucket = bucket, file = analyses_path$local)
      } else {
        warning(analyses_path$local, " does not exist for this run")
      }
    }
  }
  tictoc::toc()
}

fetch_analyses(params$run_id, params$run_id_year, analyses_paths)

# Define the base directory
base_dir <- "analyses/new_run"

# Function to generate new file path using run_id
generate_new_path <- function(file_type, run_id) {
  file.path(base_dir, paste0(run_id, "-", file_type, ".parquet"))
}

shap_df <- read_parquet(generate_new_path("shap", params$run_id))

model_performance_assessment <- arrow::read_parquet(generate_new_path("performance", params$run_id))

assessment_card <- read_parquet(generate_new_path("assessment_card", params$run_id)) %>%
    select(meta_pin, meta_card_num, pred_card_initial_fmv)

metadata <- read_parquet(generate_new_path("metadata", params$run_id))

assessment_pin <- read_parquet(generate_new_path("assessment_pin", params$run_id))

lockfile <- metadata$dvc_md5_assessment_data

s3_path <- paste0("s3://ccao-data-dvc-us-east-1/files/md5/",
                  substr(lockfile, 1, 2), "/",
                  substr(lockfile, 3, nchar(lockfile)))

save_object(object = s3_path, file = "analyses/new_run/assessment_data.parquet")

assessment_data <- read_parquet("analyses/new_run/assessment_data.parquet")

```

```{r download_comparison_data, echo = FALSE}

analyses_paths <- list(
  output = list(
    list(
      s3 = paste0("s3://ccao-model-results-us-east-1/performance/year=", params$comparison_run_id_year, "/stage=assessment/", params$comparison_run_id, ".parquet"),
      local = file.path("analyses", "comparison_run", paste0(params$comparison_run_id, "-performance.parquet"))
    ),
    list(
      s3 = paste0("s3://ccao-model-results-us-east-1/assessment_pin/year=", params$comparison_run_id_year, "/run_id=", params$comparison_run_id, "/"),
      local = file.path("analyses", "comparison_run", paste0(params$comparison_run_id, "-assessment_pin.parquet"))
  ),
    list(
      s3 = paste0("s3://ccao-model-results-us-east-1/metadata/year=", params$comparison_run_id_year, "/", params$comparison_run_id, ".parquet"),
      local = file.path("analyses", "comparison_run", paste0(params$comparison_run_id, "-metadata.parquet"))
    )
  )
)

fetch_analyses(params$comparison_run_id, params$comparison_run_id_year, analyses_paths)

base_dir <- "analyses/comparison_run"

metadata_comparison <- read_parquet(generate_new_path("metadata", params$comparison_run_id))

model_performance_assessment_comparison <- arrow::read_parquet(generate_new_path("performance", params$comparison_run_id))

assessment_pin_comparison <- arrow::read_parquet(generate_new_path("assessment_pin", params$comparison_run_id))
```



```{r data_processing, echo = FALSE}
assessment_data_small <- assessment_data %>%
  select(meta_pin, meta_card_num, meta_nbhd_code, loc_longitude, loc_latitude, !!sym(params$added_variable))

# Process the working data
working_data <- shap_df %>%
  select(meta_pin, meta_card_num, pred_card_shap_baseline_fmv, !!sym(params$added_variable)) %>%
  rename(!!params$added_variable_shap := !!sym(params$added_variable)) %>%
  inner_join(assessment_data_small, by = c("meta_pin", "meta_card_num")) %>%
  inner_join(assessment_card, by = c("meta_pin", "meta_card_num")) %>%
  group_by(meta_nbhd_code) %>%
  mutate(
    !!paste0(params$added_variable, "_shap_neighborhood_mean") := mean(abs(!!sym(params$added_variable_shap)), na.rm = TRUE),
    !!paste0(params$added_variable, "_shap_neighborhood_90th") := quantile(abs(!!sym(params$added_variable_shap)), 0.9, na.rm = TRUE),
    !!paste0(params$added_variable, "_neighborhood_mean") := mean(!!sym(params$added_variable), na.rm = TRUE),
    !!paste0(params$added_variable, "_neighborhood_median") := median(!!sym(params$added_variable), na.rm = TRUE),
    !!paste0(params$added_variable, "_neighborhood_90th") := quantile(!!sym(params$added_variable), 0.9, na.rm = TRUE),
    median_card_value = median(pred_card_initial_fmv, na.rm = TRUE)
  ) %>%
  ungroup()


nbhd <- ccao::nbhd_shp

spatial_data <- working_data %>%
  distinct(meta_nbhd_code, .keep_all = TRUE) %>%
  inner_join(nbhd, by = c("meta_nbhd_code" = "town_nbhd")) %>%
  st_as_sf()

```

# Descriptive Statistics

```{r Mean, echo = FALSE}
descriptives <- working_data %>%
  mutate(
    mean = mean(!!sym(params$added_variable), na.rm = TRUE),
    median = median(!!sym(params$added_variable), na.rm = TRUE)
  )
```

The mean of the data is `r round(descriptives$mean, 2)` and the median is `r round(descriptives$median, 2)`. The histogram below, shows the distribution of data in relation to both the mean and the median values.

```{r Histogram, echo = FALSE}
ggplot(working_data, aes(x = !!sym(params$added_variable))) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(aes(xintercept = mean(!!sym(params$added_variable), na.rm = TRUE)),
             color = "red", linetype = "dashed", linewidth = 1, show.legend = TRUE) +
  geom_vline(aes(xintercept = median(!!sym(params$added_variable), na.rm = TRUE)),
             color = "green", linetype = "dashed", linewidth = 1, show.legend = TRUE) +
  labs(title = paste("Histogram of", params$added_variable),
       x = params$added_variable,
       y = "Frequency") +
  theme_minimal() +
  annotate("text", x = mean(descriptives$mean), y = Inf, label = "Mean", color = "red", vjust = 1.5, hjust = -3) +
  annotate("text", x = mean(descriptives$median), y = Inf, label = "Median", color = "green", vjust = 1.5, hjust = -1)
```


## Correlation between Added Variable and Other Variables

While tree-based models are not particularly sensitive to multicollinearity, but it is still important to understand the relationship between the new variable and the existing variables. In this table, we should be looking to see if we can identify another variable which *very* neatly aligns with other existing variables. Columns are produced with both the absolute value of the correlation (for easy sorting), as well as the correlation to help decipher the direction of the relationship.

```{r, echo = FALSE}
columns_to_remove <- c(
  "time_sale_year", 
  "time_sale_month_of_year", 
  "time_sale_day_of_year", 
  "time_sale_day_of_week", 
  "time_sale_day_of_month", 
  "time_sale_day"
)

if (params$type == "continuous") {
  # Select only numeric columns from assessment_data and remove the specified columns
  numeric_cols <- assessment_data %>%
    select_if(is.numeric) %>%
    select(-all_of(columns_to_remove))
  
  # Initialize a data frame to store correlation results
  correlation_results <- data.frame(Column = character(), Correlation = numeric(), Abs_Correlation = numeric(), stringsAsFactors = FALSE)
  
  # Loop through each numeric column and calculate correlation and absolute correlation
  for (col_name in names(numeric_cols)) {
    correlation_value <- cor(numeric_cols[[col_name]], assessment_data[[params$added_variable]], use = "complete.obs")
    abs_correlation_value <- abs(cor(abs(numeric_cols[[col_name]]), abs(assessment_data[[params$added_variable]]), use = "complete.obs"))
    correlation_results <- rbind(correlation_results, data.frame(Column = col_name, Correlation = correlation_value, Abs_Correlation = abs_correlation_value))
  }
  
  # Sort the correlation results in descending order by Correlation
  correlation_results <- correlation_results %>%
    arrange(dplyr::desc(Correlation))
  
  # Display the correlation results as a scrollable table
  datatable(correlation_results, options = list(scrollX = TRUE, pageLength = 10, order = list(list(1, 'desc'))))
} else {
  print(paste("assessment_data$", params$added_variable, " is not numeric.", sep = ""))
}
```

# Overview of Model Metrics

```{r Stats, echo = FALSE}
# Function to calculate the percentage difference
percentage_diff <- function(x, y) {
  ((y - x) / x) * 100
}

# Rename columns in the new assessment data
model_performance_assessment_new <- model_performance_assessment %>%
  rename_with(~ paste0(., "_new"), -c(geography_id, geography_type, class))

# Join, calculate percentage differences, filter, reorder columns, and display in a sortable and scrollable table
result <- inner_join(model_performance_assessment_comparison, model_performance_assessment_new, by = c("geography_id", "geography_type", "class")) %>%
  filter(
    geography_type %in% c("triad_code", "township_code", "nbhd_code")
  ) %>%
  select(geography_id, geography_type, class, rmse, rmse_new, mki, mki_new, cod, cod_new, prb, prb_new, prd, prd_new, r_squared, r_squared_new) %>%
  mutate(
    rmse_pct_diff = percentage_diff(rmse, rmse_new),
    mki_pct_diff = percentage_diff(mki, mki_new),
    cod_pct_diff = percentage_diff(cod, cod_new),
    prb_pct_diff = percentage_diff(prb, prb_new),
    prd_pct_diff = percentage_diff(prd, prd_new),
    r_squared_pct_diff = percentage_diff(r_squared, r_squared_new)
  ) %>%
  select(
    geography_id, geography_type, class,
    rmse, rmse_new, rmse_pct_diff,
    mki, mki_new, mki_pct_diff,
    cod, cod_new, cod_pct_diff,
    prb, prb_new, prb_pct_diff,
    prd, prd_new, prd_pct_diff,
    r_squared, r_squared_new, r_squared_pct_diff
  ) %>%
  mutate(across(where(is.numeric), \(x) round(x, 2)))

output <- result %>%
  filter(is.na(class)) %>%
  filter(is.na(geography_id)) %>%
  select(rmse, rmse_new)

```

Before running this report, make sure that the lightgbm model is set to optimize the Root Mean Square Error (RMSE). This means that the addition of a new variable *should* improve this metric. In this situation, the model sees a reduction of `r output$rmse` to `r output$rmse_new`, a change of `r output$rmse_new - output$rmse`.

RSME only describes some of the metrics that we are interested in, and it doesn't take into account changes that occur on more local levels. The tables below are split into three different geographies, triad, township, and neighborhood. Here, you can see the

```{r, echo = FALSE}
# Display the result as a sortable and scrollable table
result_list <- split(result, result$geography_type)
```

```{r, echo = FALSE}
# Create a list of datatables, one for each geography_type
datatable_list <- lapply(names(result_list), function(geo_type) {
  datatable(result_list[[geo_type]], options = list(scrollX = TRUE, pageLength = 10, order = list(list(1, 'desc'))),
            caption = paste("Geography Type:", geo_type))
})
```

## Assessor Metrics at Different Geographies

### Triad

```{r, echo = FALSE}
datatable_list[3]

```

### Township

```{r, echo = FALSE}
datatable_list[2]

```

### Neighborhood

```{r, echo = FALSE}
datatable_list[1]

```


# SHAP
The main metric that the CCAO Data team uses to assess the importance of a variable is the SHAP value. SHAP values are a way to explain the output of machine learning models. They assign the $value increase associated with each variable. The SHAP value is calculated for each observation in the dataset, and the median SHAP value is used to determine the importance of a variable. The higher the median SHAP value, the more important the variable is in the model.

## Absolute Value Rank of SHAP Scores

The following table produces the median absolute SHAP value by township, and creates a . In total, there are `r length(shap_predictors)` indicators in the model. Thus, if the median SHAP is ranked 1, it is the most important feature in a township, while if it is ranked `r length(shap_predictors)`, it is the least important feature in a township. The median value (without absolute) is also included to better contextualize the impact.

```{r, echo = FALSE}
shap_predictors <- unlist(metadata$model_predictor_all_name)

shap_df_filtered_long <- shap_df %>%
  inner_join(
    assessment_data %>%
      select(meta_pin, meta_card_num, meta_township_code, meta_nbhd_code) %>%
      rename(township_code = meta_township_code, neighborhood_code = meta_nbhd_code), 
    by = c("meta_pin", "meta_card_num")
  ) %>%
  mutate(other_affordability_risk_index = 1) %>%
  select(township_code, neighborhood_code, all_of(shap_predictors)) %>%
  pivot_longer(
    cols = all_of(shap_predictors),
    names_to = "feature",
    values_to = "shap"
  )

shap_predictors <- unlist(metadata$model_predictor_all_name)

shap_df_filtered_long <- shap_df %>%
  mutate(other_affordability_risk_index = 1) %>%
  inner_join(
    assessment_data %>%
      select(meta_pin, meta_card_num, meta_township_code, meta_nbhd_code) %>%
      rename(township_code = meta_township_code, neighborhood_code = meta_nbhd_code), 
    by = c("meta_pin", "meta_card_num")
  ) %>%
  select(township_code, all_of(shap_predictors)) %>%
  pivot_longer(
    cols = all_of(shap_predictors),
    names_to = "feature",
    values_to = "shap"
  ) %>%
  group_by(township_code, feature) %>%
  mutate(median_abs_shap = median(abs(shap)),
         median_shap = median(shap)) %>%
  distinct(township_code, feature, .keep_all = TRUE)

ranked_shap_df <- shap_df_filtered_long %>%
  group_by(township_code) %>%
  arrange(desc(median_abs_shap), .by_group = TRUE) %>%
  mutate(rank_abs = row_number()) %>%
  arrange(desc(median_shap), .by_group = TRUE) %>%
  mutate(rank = row_number()) %>%
  ungroup()

create_kable_chart <- function(township_data, added_variable) {
  added_variable_data <- township_data %>%
    filter(feature == added_variable)
  
  if(nrow(added_variable_data) == 0) {
    return(NULL)  
  }
  
  non_added_variable_data <- township_data %>%
    filter(feature != added_variable)
  
  combined_data <- bind_rows(added_variable_data, non_added_variable_data)
  
  kable_chart <- combined_data %>%
    select(feature, median_abs_shap, median_shap, rank_abs) %>%
    mutate(across(where(is.numeric), round, 2))
  
  datatable_chart <- datatable(kable_chart, 
                               caption = paste("Township Code:", unique(township_data$township_code)), 
                               options = list(scrollY = '300px', 
                                              paging = FALSE, 
                                              searching = FALSE))
  
  return(datatable_chart)
}

township_codes <- unique(ranked_shap_df$township_code)
added_variable <- params$added_variable

kable_charts <- lapply(township_codes, function(code) {
  township_data <- ranked_shap_df %>%
    filter(township_code == code)
  create_kable_chart(township_data, added_variable)
})

for (chart in kable_charts) {
  if (!is.null(chart)) {
    print(chart)
  }
}

```


## Correlation between SHAP and the Added Variable

```{r, echo = FALSE}
correlation_value <- cor(pull(working_data, params$added_variable_shap), pull(working_data, params$added_variable), use = "complete.obs")
```

One way to test if the added feature is improving the model, is to see if there is a relationship between the SHAP values and the added values. The assumption would be, that if the added value caused an increase / decrease in assessed values, then the correlation would be high in a positive or negative value. In this case, the correlation between the SHAP values and the added variable is `r round(correlation_value, 2)`.

# Spatial Analysis

The spatial analysis is broken up into a few sections. The first panel looks at 


## Neighborhood Level Mean of `r params$added_variable`

```{r, echo = FALSE}
if (params$type == "continuous") {
  plot <- spatial_data %>%
    ggplot() +
    geom_sf(aes(fill = !!sym(paste0(params$added_variable, "_neighborhood_mean")))) +
    scale_fill_viridis_c(option = "viridis", name = "Value") +
    theme_void() +
    coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
  
  print(plot)
} else {
  message("The added_variable is not continuous. Skipping the plot.")
}
```

## Neighborhood Level Mean Absolute Value of SHAP

```{r, echo = FALSE}
spatial_data %>%
  ggplot() +
  geom_sf(aes(fill = !!sym(paste0(params$added_variable_shap, "_neighborhood_mean")))) +
  scale_fill_viridis_c(option = "viridis", name = "Value") +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```

## Neighborhood Level Absolute Value 90th Percentile of SHAP

Sometimes the value of SHAPS lie in the extremes, rather than the

```{r, echo = FALSE}
spatial_data %>%
  ggplot() +
  geom_sf(aes(fill = !!sym(paste0(params$added_variable_shap, "_neighborhood_90th")))) +
  scale_fill_viridis_c(option = "viridis", name = "Value") +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```




## Mapping the Highest and Lowest Values

```{r, echo = FALSE}
# Assuming 'highest_100' is already defined
highest_100 <- working_data %>%
  arrange(desc(!!sym(params$added_variable))) %>%
  slice(1:100)

# Define a color palette
pal_highest_100 <- colorNumeric(palette = "viridis", domain = highest_100[[params$added_variable]])

# Create the leaflet map
leaflet(highest_100) %>%
  addTiles() %>%
  addCircleMarkers(
    lng = ~loc_longitude,
    lat = ~loc_latitude,
    radius = 5,
    color = ~ pal_highest_100(highest_100[[params$added_variable]]),
    popup = ~paste(
      "<br>", "SHAP:", sprintf("%.2f", highest_100[[params$added_variable_shap]]),
      "<br>", "Variable:", sprintf("%.2f", highest_100[[params$added_variable]]),
      "<br>", "Pin: ", highest_100$meta_pin
    )
  ) %>%
  addLegend(
    "bottomright",
    pal = pal_highest_100,
    values = ~ highest_100[[params$added_variable]],
    title = "Legend (Highest 100)"
  )


```

```{r, echo = FALSE}
# Assuming 'lowest_100' is already defined
lowest_100 <- working_data %>%
  arrange(!!sym(params$added_variable)) %>%
  slice(1:100)

# Define a color palette
pal_lowest_100 <- colorNumeric(palette = "viridis", domain = lowest_100[[params$added_variable]])

# Create the leaflet map
leaflet(lowest_100) %>%
  addTiles() %>%
  addCircleMarkers(
    lng = ~loc_longitude,
    lat = ~loc_latitude,
    radius = 5,
    color = ~ pal_lowest_100(lowest_100[[params$added_variable]]),
    popup = ~paste(
      "<br>", "SHAP:", sprintf("%.2f", lowest_100[[params$added_variable_shap]]),
      "<br>", "Variable:", sprintf("%.2f", lowest_100[[params$added_variable]]),
      "<br>", "Pin: ", lowest_100$meta_pin
    )
  ) %>%
  addLegend(
    "bottomright",
    pal = pal_lowest_100,
    values = ~ lowest_100[[params$added_variable]],
    title = "Legend (Lowest 100)"
  )

```

## Neighborhoods with the Highest and Lowest SHAP Values

```{r, echo = FALSE}
working_data <- working_data %>%
  group_by(meta_nbhd_code) %>%
  mutate(
    mean_value = mean(abs(!!sym(paste0(params$added_variable_shap))), na.rm = TRUE)
  ) %>%
  ungroup()

selected_neighborhoods <- working_data %>%
  distinct(meta_nbhd_code, .keep_all = TRUE) %>%
  arrange(mean_value) %>%
  slice(c(1:2, (n() - 1):n())) %>%
  pull(meta_nbhd_code)

# Filter the data to include only the selected neighborhoods
selected_data <- working_data %>%
  filter(meta_nbhd_code %in% selected_neighborhoods)

# Separate high and low standard deviation neighborhoods
high_mean_data <- selected_data %>%
  filter(meta_nbhd_code %in% selected_neighborhoods[(length(selected_neighborhoods) - 1):length(selected_neighborhoods)])

low_mean_data <- selected_data %>%
  filter(meta_nbhd_code %in% selected_neighborhoods[1:2])

# Define color palettes for each subset
pal_high_mean <- colorNumeric(
  palette = "viridis",
  domain = high_mean_data[[paste0(params$added_variable_shap)]]
)

pal_low_mean <- colorNumeric(
  palette = "viridis",
  domain = low_mean_data[[paste0(params$added_variable_shap)]]
)
```

```{r, echo = FALSE}
# Create the leaflet map
leaflet(low_mean_data) %>%
  addTiles() %>%
  addCircleMarkers(
    lng = ~loc_longitude,
    lat = ~loc_latitude,
    radius = 5,
    color = ~ pal_low_mean(low_mean_data[[paste0(params$added_variable_shap)]]),
    popup = ~paste(
      "<br>", "SHAP:", sprintf("%.2f", low_mean_data[[params$added_variable_shap]]),
      "<br>", "Variable:", sprintf("%.2f", low_mean_data[[params$added_variable]]),
      "<br>", "Pin: ", low_mean_data$meta_pin)) %>%
  addLegend(
    "bottomright",
    pal = pal_low_mean,
    values = ~ low_mean_data[[paste0(params$added_variable_shap)]],
    title = "Legend (Low mean Neighborhoods)"
  )

```

## Neighborhoods with the 2 Highest Absolute Mean Values

```{r, echo = FALSE}
leaflet(high_mean_data) %>%
  addTiles() %>%
  addCircleMarkers(
    lng = ~loc_longitude,
    lat = ~loc_latitude,
    radius = 5,
    color = ~ pal_high_mean(high_mean_data[[paste0(params$added_variable_shap)]]),
    popup = ~paste(
      "<br>", "SHAP:", sprintf("%.2f", high_mean_data[[params$added_variable_shap]]),
      "<br>", "Variable:", sprintf("%.2f", high_mean_data[[params$added_variable]]),
      "<br>", "Pin: ", high_mean_data$meta_pin)) %>%
  addLegend(
    "bottomright",
    pal = pal_high_mean,
    values = ~ high_mean_data[[paste0(params$added_variable_shap)]],
    title = "Legend (High mean Neighborhoods)"
  )
```

## Relative Absolute Values

Because some neighborhoods 

```{r, echo = FALSE}
top_10_data_relative <- working_data %>%
  # mutate(shap_relative_value = abs((!!sym(paste0(params$added_variable_shap)) - pred_card_shap_baseline_fmv) / median_card_value)) %>%
  mutate(shap_relative_value = !!sym(params$added_variable_shap) / median_card_value) %>%
  group_by(meta_nbhd_code) %>%
  top_n(10, wt = shap_relative_value) %>%
  ungroup()

pal_top_10_data_relative <- colorNumeric(
  palette = "viridis",
  domain = top_10_data_relative$shap_relative_value
)

# Create the leaflet map with top 10 relative SHAP values
leaflet(top_10_data_relative) %>%
  addTiles() %>%
  addCircleMarkers(
    lng = ~loc_longitude,
    lat = ~loc_latitude,
    radius = 5,
    color = ~pal_top_10_data_relative(shap_relative_value),
    popup = ~paste(
      "SHAP Relative Value:", sprintf("%.2f", shap_relative_value),
      "<br>", "SHAP:", sprintf("%.2f", top_10_data_relative[[params$added_variable_shap]]),
      "<br>", "Variable:", sprintf("%.2f", top_10_data_relative[[params$added_variable]]),
      "<br>", "Pin: ", top_10_data_relative$meta_pin)) %>%
  addLegend(
    "bottomright",
    pal = pal_top_10_data_relative,
    values = ~shap_relative_value,
    title = "SHAP Relative Value"
  )
```

```{r, echo = FALSE}
top_10_data <- working_data %>%
  group_by(meta_nbhd_code) %>%
  top_n(10, wt = !!sym(paste0(params$added_variable_shap))) %>%
  ungroup()

pal_top_10 <- colorNumeric(
  palette = "viridis",
  domain = top_10_data[[params$added_variable_shap]]
)

leaflet(top_10_data) %>%
  addTiles() %>%
  addCircleMarkers(
    lng = ~loc_longitude,
    lat = ~loc_latitude,
    radius = 5,
    color = ~ pal_top_10(top_10_data[[params$added_variable_shap]]),
    popup = ~ paste(
      "SHAP:", sprintf("%.2f", top_10_data[[params$added_variable_shap]]),
      "<br>", "Variable:", sprintf("%.2f", top_10_data[[params$added_variable]]),
      "<br>", "Pin: ", top_10_data$meta_pin
    )
  ) %>%
  addLegend(
    "bottomright",
    pal = pal_top_10,
    values = ~ top_10_data[[params$added_variable_shap]],
    title = "Legend (Top 10 SHAP Values per Neighborhood)"
  )
```

```{r, echo = FALSE}
bottom_10_data <- working_data %>%
  group_by(meta_nbhd_code) %>%
  top_n(-10, wt = !!sym(paste0(params$added_variable_shap))) %>%
  ungroup()

# Define the color palette
pal_bottom_10 <- colorNumeric(
  palette = "viridis",
  domain = bottom_10_data[[params$added_variable_shap]]
)

# Create the leaflet map with bottom 10 values
leaflet(bottom_10_data) %>%
  addTiles() %>%
  addCircleMarkers(
    lng = ~loc_longitude,
    lat = ~loc_latitude,
    radius = 5,
    color = ~ pal_bottom_10(bottom_10_data[[params$added_variable_shap]]),
    popup = ~ paste(
      "SHAP:", sprintf("%.2f", bottom_10_data[[params$added_variable_shap]]),
      "<br>", "Variable:", sprintf("%.2f", bottom_10_data[[params$added_variable]]),
      "<br>", "Pin: ", bottom_10_data$meta_pin
    )
  ) %>%
  addLegend(
    "bottomright",
    pal = pal_bottom_10,
    values = ~ bottom_10_data[[params$added_variable_shap]],
    title = "Bottom 10 SHAP Values per Neighborhood"
  )
```


# Deciding on the Use of a New Variable

Feature selection for the lightgbm model is not as straightforward as selecting the statistical significant values or the using dimension reduction techniques to combine similar variables. Because of this, this guide can be used as an alternative where

To remove a variable from the model, it must satisfy the following criteria: 
- The variable must have a low SHAP value across all townships 
- The model metrics must not be significantly impacted by the removal of the variable 
- The variable must not demonstrate localized effects

Other metrics can be used to guide this, but are not a pre-requisite 

- There must be little correlation between SHAP values and the variable 
- The variable is highly correlated with another existing variable.
