---
title: "New Feature Template"
subtitle: "Run ID: `r params$added_feature`: `r params$description`"
date: "`r Sys.Date()`"
author: "Cook County Assessor's Office Data Department"
execute:
  echo: false
  warning: false
  asis: true
format:
  html:
    embed-resources: true
    toc: true
    toc_float: true
    fig-align: center
    fontsize: 12pt
params:
  run_id: "2024-07-03-charming-boni"
  run_id_year: "2024"
  comparison_run_id: "2024-07-09-cool-takuya"
  comparison_run_id_year: "2024"
  added_feature: "prox_nearest_new_construction_dist_ft" 
  added_feature_shap: "prox_nearest_new_construction_dist_ft_shap"
  description: "A distance in feet to the nearest new construction"
  min_range: 5
  max_range: 95
  type: "continuous"
---

```{r packages}
library(purrr)
library(here)
library(yaml)
library(corrplot)
# Load list of helper files and main libraries
purrr::walk(list.files(here::here("R"), "\\.R$", full.names = TRUE), source)

# Load reporting-only R libraries
suppressPackageStartupMessages({
  reporting_libs <- "Config/renv/profiles/reporting/dependencies"
  purrr::walk(
    strsplit(read_yaml(here::here("DESCRIPTION"))[[reporting_libs]], ", ")[[1]],
    library,
    character.only = TRUE
  )
})

# TODO: Catch for weird Arrow bug with SIGPIPE. Need to permanently fix later
# https://github.com/apache/arrow/issues/32026
cpp11::cpp_source(code = "
#include <csignal>
#include <cpp11.hpp>

[[cpp11::register]] void ignore_sigpipes() {
  signal(SIGPIPE, SIG_IGN);
}
")

ignore_sigpipes()
```

```{r download_new_data}
base_paths <- model_file_dict(params$run_id, params$run_id_year)
comparison_paths <- model_file_dict(
  params$comparison_run_id,
  params$comparison_run_id_year
)
run_id <- params$run_id
comparison_run_id <- params$comparison_run_id

analyses_paths <- list(
  output = list(
    list(
      s3 = base_paths$output$assessment_card$s3,
      key = "assessment_card"
    ),
    list(
      s3 = base_paths$output$assessment_pin$s3,
      key = "assessment_pin"
    ),
    list(
      s3 = base_paths$output$metadata$s3,
      key = "metadata"
    ),
    list(
      s3 = base_paths$output$performance_test$s3,
      key = "performance_test"
    ),
    list(
      s3 = base_paths$output$shap$s3,
      key = "shap"
    )
  )
)

source("analyses/Analysis_helpers.R")
data_new <- model_fetch_run_subset(
  params$run_id,
  params$run_id_year,
  analyses_paths, TRUE
)

list2env(data_new, envir = .GlobalEnv)

rm(data_new)

comparison_paths <- list(
  output = list(
    list(
      s3 = base_paths$output$assessment_card$s3,
      key = "assessment_card"
    ),
    list(
      s3 = base_paths$output$assessment_pin$s3,
      key = "assessment_pin"
    ),
    list(
      s3 = base_paths$output$metadata$s3,
      key = "metadata"
    ),
    list(
      s3 = base_paths$output$performance_test$s3,
      key = "performance_test"
    ),
    list(
      s3 = base_paths$output$shap$s3,
      key = "shap"
    )
  )
)

data_comparison <- model_fetch_run_subset(
  params$comparison_run_id,
  params$comparison_run_id_year,
  comparison_paths, TRUE
)

list2env(data_comparison, envir = .GlobalEnv)

rm(data_comparison)

all_vars <- ls()

# Iterate over all variables and rename if necessary
for (var_name in all_vars) {
  rename_var(var_name, params$run_id, "_new")
  rename_var(var_name, params$comparison_run_id, "_comparison")
}


lockfile_assessment <- metadata_new$dvc_md5_assessment_data

# Define S3 paths for assessment' data
s3_path_assessment <- paste0(
  "s3://ccao-data-dvc-us-east-1/files/md5/",
  substr(lockfile_assessment, 1, 2), "/",
  substr(lockfile_assessment, 3, nchar(lockfile_assessment))
)

assessment_data_new <- read_parquet(s3_path_assessment)
```

```{r download_comparison_data}
metadata_comparison <- data_comparison$metadata
model_performance_assessment_comparison <- data_comparison$performance
shap_comparison <- data_comparison$shap

assessment_pin_comparison <- data_comparison$assessment_pin %>%
  select(meta_pin, pred_pin_final_fmv, sale_ratio_study_price, meta_nbhd_code, meta_triad_code, pred_pin_initial_fmv, meta_township_code)
```

```{r}
# write_parquet(assessment_data, "assessment_data.parquet")
# write_parquet(assessment_pin, "assessment_pin.parquet")
# write_parquet(assessment_card, "assessment_card.parquet")
# write_parquet(assessment_card_full, "assessment_card_full.parquet")
# write_parquet(performance, "performance.parquet")
# write_parquet(shap, "shap.parquet")
# write_parquet(metadata, "metadata.parquet")
# write_parquet(assessment_pin_comparison, "assessment_pin_comparison.parquet")
# write_parquet(model_performance_assessment_comparison, "model_performance_assessment_comparison.parquet")
# write_parquet(shap_comparison, "shap_comparison.parquet")
# write_parquet(metadata_comparison, "metadata_comparison.parquet")
```

```{r}
assessment_data <- read_parquet("assessment_data.parquet")
assessment_pin <- read_parquet("assessment_pin.parquet")
assessment_card <- read_parquet("assessment_card.parquet")
assessment_card_full <- read_parquet("assessment_card_full.parquet")
performance <- read_parquet("performance.parquet")
shap <- read_parquet("shap.parquet")
metadata <- read_parquet("metadata.parquet")
assessment_pin_comparison <- read_parquet("assessment_pin_comparison.parquet")
model_performance_assessment_comparison <- read_parquet("model_performance_assessment_comparison.parquet")
shap_comparison <- read_parquet("shap_comparison.parquet")
metadata_comparison <- read_parquet("metadata_comparison.parquet")
```

```{r}
target_feature_value <- params$added_feature
target_feature_shap <- params$added_feature_shap
nbhd <- ccao::nbhd_shp

# Selecting and joining relevant data
card_individual <- shap %>%
  select(meta_pin, meta_card_num, pred_card_shap_baseline_fmv, {{ target_feature_value }}) %>%
  rename(!!target_feature_shap := !!target_feature_value) %>%
  inner_join(assessment_card, by = c("meta_pin", "meta_card_num")) %>%
  inner_join(
    assessment_data %>%
      select(meta_pin, meta_card_num, meta_nbhd_code, loc_longitude, loc_latitude, meta_township_name),
    by = c("meta_pin", "meta_card_num")
  )


# Summarizing data by neighborhood code
card_nbhd <- card_individual %>%
  group_by(meta_nbhd_code) %>%
  summarize(
    avg_target_feature_shap = mean(!!sym({{ target_feature_shap }}), na.rm = TRUE),
    avg_pred_card_shap_baseline_fmv = mean(pred_card_shap_baseline_fmv, na.rm = TRUE)
  ) %>%
  ungroup()


pin_individual <- assessment_pin %>%
  select(meta_pin, pred_pin_final_fmv, pred_pin_initial_fmv) %>%
  rename(
    pred_pin_final_fmv_new = pred_pin_final_fmv,
    pred_pin_initial_fmv_new = pred_pin_initial_fmv
  ) %>%
  inner_join(
    assessment_pin_comparison %>%
      select(meta_pin, pred_pin_final_fmv, pred_pin_initial_fmv),
    by = "meta_pin"
  ) %>%
  rename(
    pred_pin_final_fmv_comp = pred_pin_final_fmv,
    pred_pin_initial_fmv_comp = pred_pin_initial_fmv
  ) %>%
  mutate(
    diff_pred_pin_final_fmv = round(((
      pred_pin_final_fmv_new - pred_pin_final_fmv_comp) / pred_pin_final_fmv_comp) * 100, 2),
    pred_pin_final_fmv_new = dollar(pred_pin_final_fmv_new),
    pred_pin_final_fmv_comp = dollar(pred_pin_final_fmv_comp),
    diff_pred_pin_initial_fmv = round(((
      pred_pin_initial_fmv_new - pred_pin_initial_fmv_comp) / pred_pin_initial_fmv_comp) * 100, 2),
    pred_pin_initial_fmv_new = dollar(pred_pin_initial_fmv_new),
    pred_pin_initial_fmv_comp = dollar(pred_pin_initial_fmv_comp)
  ) %>%
  inner_join(
    assessment_data %>%
      select(meta_pin, meta_nbhd_code, loc_longitude, loc_latitude, meta_township_name, {{ target_feature_value }}),
    by = "meta_pin"
  )

pin_nbhd <- pin_individual %>%
  group_by(meta_nbhd_code) %>%
  summarize(
    !!paste0({{ target_feature_value }}, "_neighborhood_mean") := mean(!!sym({{ target_feature_value }}), na.rm = TRUE),
    !!paste0({{ target_feature_value }}, "_neighborhood_median") := median(!!sym({{ target_feature_value }}), na.rm = TRUE),
    !!paste0({{ target_feature_value }}, "_neighborhood_90th") := quantile(!!sym({{ target_feature_value }}), 0.9, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  inner_join(
    nbhd,
    by = c("meta_nbhd_code" = "town_nbhd")
  )
```


```{r data_processing}
assessment_data_small <- assessment_data %>%
  select(meta_pin, meta_card_num, meta_nbhd_code, loc_longitude, loc_latitude, meta_township_name, !!sym(params$added_feature))

# Create a card level dataset
working_data_card <- shap %>%
  select(meta_pin, meta_card_num, pred_card_shap_baseline_fmv, !!sym(params$added_feature)) %>%
  rename(!!params$added_feature_shap := !!sym(params$added_feature)) %>%
  inner_join(assessment_card, by = c("meta_pin", "meta_card_num")) %>%
  rename(added_feature_card = !!sym(params$added_feature)) %>%
  inner_join(
    assessment_data %>%
      select(meta_pin, meta_card_num, meta_nbhd_code, loc_longitude, loc_latitude, meta_township_name, !!sym(params$added_feature)),
    by = c("meta_pin", "meta_card_num")
  ) %>%
  group_by(meta_nbhd_code) %>%
  mutate(
    !!paste0(params$added_feature, "_shap_neighborhood_mean") := mean(abs(!!sym(params$added_feature_shap)), na.rm = TRUE),
    !!paste0(params$added_feature, "_shap_neighborhood_90th") := quantile(abs(!!sym(params$added_feature_shap)), 0.9, na.rm = TRUE),
    median_card_value = median(pred_card_initial_fmv, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  inner_join(assessment_pin %>% select(meta_pin, pred_pin_final_fmv, pred_pin_initial_fmv), by = "meta_pin") %>%
  rename(
    pred_pin_final_fmv_new = pred_pin_final_fmv,
    pred_pin_initial_fmv_new = pred_pin_initial_fmv
  ) %>%
  inner_join(assessment_pin_comparison %>% select(meta_pin, pred_pin_final_fmv, pred_pin_initial_fmv), by = "meta_pin") %>%
  rename(
    pred_pin_final_fmv_comp = pred_pin_final_fmv,
    pred_pin_initial_fmv_comp = pred_pin_initial_fmv
  ) %>%
  mutate(
    shap_relative = percent((!!sym(params$added_feature_shap) / pred_card_initial_fmv), accuracy = 0.01),
    diff_pred_pin_final_fmv = round(pred_pin_final_fmv_new - pred_pin_final_fmv_comp, 2),
    pred_pin_final_fmv_new = scales::dollar(pred_pin_final_fmv_new),
    pred_pin_final_fmv_comparison = scales::dollar(pred_pin_final_fmv_comp),
    diff_pred_pin_initial_fmv = round(pred_pin_initial_fmv_new - pred_pin_initial_fmv_comp, 2),
    pred_pin_initial_fmv_new = scales::dollar(pred_pin_initial_fmv_new),
    pred_pin_initial_fmv_comp = scales::dollar(pred_pin_initial_fmv_comp)
  )

temp_pin <- working_data_card %>%
  distinct(meta_pin, .keep_all = TRUE) %>%
  group_by(meta_nbhd_code) %>%
  mutate(
    !!paste0(params$added_feature, "_neighborhood_mean") := mean(!!sym(params$added_feature), na.rm = TRUE),
    !!paste0(params$added_feature, "_neighborhood_median") := median(!!sym(params$added_feature), na.rm = TRUE),
    !!paste0(params$added_feature, "_neighborhood_90th") := quantile(!!sym(params$added_feature), 0.9, na.rm = TRUE)
  ) %>%
  ungroup()

# Join the temporary dataset back to the original dataset
working_data_card <- working_data_card %>%
  left_join(temp_pin, by = "meta_nbhd_code")

# Filter to pin for clarity in later work
working_data_pin <- working_data_card %>%
  distinct(meta_pin, .keep_all = TRUE)

nbhd <- ccao::nbhd_shp

spatial_data <- working_data_card %>%
  distinct(meta_nbhd_code, .keep_all = TRUE) %>%
  inner_join(nbhd, by = c("meta_nbhd_code" = "town_nbhd")) %>%
  st_as_sf()
```

# Descriptive Statistics

Based on the params$type feature, charts with the mean and median values of the feature are displayed for continuous features, and the percentage of each category is displayed for categorical features. These tables are broken down by township and neighborhood.


::: {.panel-tabset}


## Overall

```{r mean_median}
if (params$type == "continuous") {
  descriptives <- working_data_pin %>%
    summarize(
      mean = round(mean(!!sym(params$added_feature), na.rm = TRUE), 2),
      median = round(median(!!sym(params$added_feature), na.rm = TRUE), 2)
    )

  datatable(descriptives,
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
} else if (params$type == "categorical") {
  category_percentages_township <- working_data_pin %>%
    count(meta_township_name, !!sym(params$added_feature)) %>%
    mutate(percentage = round(n / sum(n) * 100, 2)) %>%
    select(meta_township_name, !!sym(params$added_feature), percentage) %>%
    pivot_wider(names_from = !!sym(params$added_feature), values_from = percentage, values_fill = list(percentage = 0))

  datatable(category_percentages_township,
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
}
```

## Township
```{r mean_median_township}
if (params$type == "continuous") {
  descriptives_township <- working_data_pin %>%
    group_by(meta_township_name) %>%
    summarize(
      mean = round(mean(!!sym(params$added_feature), na.rm = TRUE), 2),
      median = round(median(!!sym(params$added_feature), na.rm = TRUE), 2)
    )

  datatable(descriptives_township,
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
} else if (params$type == "categorical") {
  category_percentages_township <- working_data_pin %>%
    group_by(meta_township_name, !!sym(params$added_feature)) %>%
    count() %>%
    group_by(meta_township_name) %>%
    mutate(percentage = round(n / sum(n) * 100, 2)) %>%
    select(meta_township_name, !!sym(params$added_feature), percentage) %>%
    pivot_wider(names_from = !!sym(params$added_feature), values_from = percentage, values_fill = list(percentage = 0))


  datatable(category_percentages_township,
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
}
```

## Neighborhood

```{r mean_median_neighborhood}
if (params$type == "continuous") {
  descriptives_nbhd <- working_data_pin %>%
    group_by(meta_nbhd_code) %>%
    summarize(
      mean = round(mean(!!sym(params$added_feature), na.rm = TRUE), 2),
      median = round(median(!!sym(params$added_feature), na.rm = TRUE), 2)
    )

  datatable(descriptives_nbhd,
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
} else if (params$type == "categorical") {
  category_percentages_nbhd <- working_data_pin %>%
    group_by(meta_nbhd_code, !!sym(params$added_feature)) %>%
    count() %>%
    mutate(percentage = n / sum(n) * 100) %>%
    select(meta_nbhd_code, !!sym(params$added_feature), percentage)

  datatable(category_percentages_nbhd,
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
}
```


:::

## Histogram

```{r histogram}
if (params$type == "continuous") {
  working_data_pin %>%
    mutate(
      mean_value = mean(!!sym(params$added_feature), na.rm = TRUE),
      median_value = median(!!sym(params$added_feature), na.rm = TRUE)
    ) %>%
    ggplot(aes(x = !!sym(params$added_feature))) +
    geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
    geom_vline(aes(xintercept = mean_value, color = "Mean"), linetype = "dashed", linewidth = 1, show.legend = TRUE) +
    geom_vline(aes(xintercept = median_value, color = "Median"), linetype = "dashed", linewidth = 1, show.legend = TRUE) +
    scale_color_manual(
      name = "Statistics",
      values = c(Mean = "red", Median = "green"),
      labels = c("Mean", "Median")
    ) +
    labs(
      x = params$added_feature,
      y = "Frequency"
    ) +
    theme_minimal()
} else if (params$type == "categorical") {
  category_percentages <- working_data_pin %>%
    count(!!sym(params$added_feature)) %>%
    mutate(percentage = n / sum(n) * 100)

  ggplot(category_percentages, aes(x = !!sym(params$added_feature), y = percentage)) +
    geom_bar(stat = "identity", fill = "blue", color = "black", alpha = 0.7) +
    labs(
      x = params$added_feature,
      y = "Percentage"
    ) +
    theme_minimal()
}
```


## FMV Change Histogram

This chart shows the distribution of the value of 'diff_pred_pin_initial_fmv' in the model with the added feature minus the model without the added feature. Outliers outside of 95% are removed to make the chart more readable. The largest 100 increases and decreases are displayed in maps in section X. 
```{r}
working_data_pin %>%
  filter(
    diff_pred_pin_initial_fmv >= quantile(diff_pred_pin_initial_fmv, 0.025, na.rm = TRUE) &
      diff_pred_pin_initial_fmv <= quantile(diff_pred_pin_initial_fmv, 0.975, na.rm = TRUE)
  ) %>%
  mutate(
    mean_value = mean(diff_pred_pin_initial_fmv, na.rm = TRUE),
    median_value = median(diff_pred_pin_initial_fmv, na.rm = TRUE)
  ) %>%
  ggplot(aes(x = diff_pred_pin_initial_fmv)) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(aes(xintercept = mean_value, color = "Mean"), linetype = "dashed", linewidth = 1, show.legend = TRUE) +
  geom_vline(aes(xintercept = median_value, color = "Median"), linetype = "dashed", linewidth = 1, show.legend = TRUE) +
  scale_color_manual(
    name = "Statistics",
    values = c(Mean = "red", Median = "green"),
    labels = c("Mean", "Median")
  ) +
  labs(
    x = "Change in FMV",
    y = "Frequency"
  ) +
  theme_minimal()
```

## SHAP Histogram

```{r}
shap %>%
  filter(
    !!sym(params$added_feature) >= quantile(!!sym(params$added_feature), 0.025, na.rm = TRUE) &
      !!sym(params$added_feature) <= quantile(!!sym(params$added_feature), 0.975, na.rm = TRUE)
  ) %>%
  mutate(
    mean_value = mean(!!sym(params$added_feature), na.rm = TRUE),
    median_value = median(!!sym(params$added_feature), na.rm = TRUE)
  ) %>%
  ggplot(aes(x = !!sym(params$added_feature))) +
  geom_histogram(fill = "blue", color = "black", alpha = 0.7) +
  geom_vline(aes(xintercept = mean_value, color = "Mean"), linetype = "dashed", linewidth = 1, show.legend = TRUE) +
  geom_vline(aes(xintercept = median_value, color = "Median"), linetype = "dashed", linewidth = 1, show.legend = TRUE) +
  scale_color_manual(
    name = "Statistics",
    values = c(Mean = "red", Median = "green"),
    labels = c("Mean", "Median")
  ) +
  labs(
    x = "SHAP Value",
    y = "Frequency"
  ) +
  theme_minimal()
```

:::

## Correlation Between Added Feature and Other Features

Here, the goal is to see if the added feature *very* neatly aligns with other existing features. Columns are produced with both the absolute value of the correlation (for easy sorting), as well as the correlation to help decipher the direction of the relationship.

```{r correlation between features}
clean_column_values <- function(df, column_name) {
  df[[column_name]] <- df[[column_name]] %>%
    gsub("^meta_|^prox_|^other_|^loc_|^char_|^acs5|^acs_|^ccao_", "", .) %>%
    gsub("_", " ", .)
  return(df)
}

columns_to_remove <- c(
  "time_sale_year",
  "time_sale_month_of_year",
  "time_sale_day_of_year",
  "time_sale_day_of_week",
  "time_sale_day_of_month",
  "time_sale_day"
)

if (params$type == "continuous") {
  # Select only numeric columns from assessment_data and remove the specified columns
  numeric_cols <- assessment_data %>%
    select_if(is.numeric) %>%
    select(-all_of(columns_to_remove))

  # Initialize a data frame to store correlation results
  correlation_results <- data.frame(Feature = character(), Correlation = numeric(), Abs_Correlation = numeric(), stringsAsFactors = FALSE)

  # Loop through each numeric column and calculate correlation and absolute correlation
  for (col_name in names(numeric_cols)) {
    # Filter out rows with missing values in the two columns being compared
    complete_cases <- complete.cases(numeric_cols[[col_name]], assessment_data[[params$added_feature]])

    # Only compute correlation if there are complete cases
    if (sum(complete_cases) > 0) {
      correlation_value <- cor(numeric_cols[[col_name]][complete_cases], assessment_data[[params$added_feature]][complete_cases], use = "complete.obs")
      abs_correlation_value <- abs(cor(abs(numeric_cols[[col_name]][complete_cases]), abs(assessment_data[[params$added_feature]][complete_cases]), use = "complete.obs"))
      correlation_results <- rbind(correlation_results, data.frame(Feature = col_name, Correlation = correlation_value, Abs_Correlation = abs_correlation_value))
    }
  }

  # Sort the correlation results in descending order by Correlation
  correlation_results <- correlation_results %>%
    arrange(dplyr::desc(Abs_Correlation)) %>%
    mutate(across(where(is.numeric), ~ round(., 2)))

  top_10_features <- correlation_results %>%
    slice(1:10) %>%
    pull(Feature)

  correlation_results_clean <- clean_column_values(correlation_results, "Feature") %>%
    slice(2:n())

  # Display the correlation results as a scrollable table
  datatable(correlation_results_clean, options = list(scrollX = TRUE, scrollY = TRUE, pageLength = 10, order = list(list(1, "desc"))))
} else {
  print(paste("assessment_data$", params$added_feature, " is not numeric.", sep = ""))
}
```

## Correlation Plot

This selects the 10 most correlated features (in terms of absolute value) from the previous chart and creates a correlation plot
```{r}
# Select the top 10 features, remove rows with NA values, rename columns, calculate the correlation, and plot the correlation matrix
assessment_data %>%
  select(all_of(top_10_features)) %>%
  na.omit() %>%
  rename_with(~ gsub("^meta_|^prox_|^other_|^loc_|^char_|^acs5|^acs_|^ccao_", "", .)) %>%
  rename_with(~ gsub("_", " ", .)) %>%
  cor() %>%
  corrplot(method = "circle", tl.cex = 0.6, tl.srt = 45, addgrid.col = "grey", mar = c(1, 1, 1, 1))
```

# Overview of Model Metrics

```{r model_stats}
# Function to calculate the percentage difference
percentage_diff <- function(x, y) {
  scales::percent((y - x) / x)
}


min_n <- 10

gte_n <- \(n_sales, min_n, fn, na_type) {
  ifelse(sum(!is.na(n_sales)) >= min_n, fn, na_type)
}
rs_fns_list <- list(
  cod_no_sop = \(x, y) gte_n(y, 2, cod(x / y, na.rm = TRUE), NA_real_),
  prd_no_sop = \(x, y) gte_n(y, 2, prd(x, y, na.rm = TRUE), NA_real_),
  prb_no_sop = \(x, y) gte_n(y, 2, prb(x, y, na.rm = TRUE), NA_real_),
  mki_no_sop = \(x, y) gte_n(y, 2, mki(x, y, na.rm = TRUE), NA_real_),
  cod = \(x, y) gte_n(y, min_n, cod(x / y, na.rm = TRUE), NA_real_),
  cod_met = \(x, y) gte_n(y, min_n, cod_met(cod(x / y, na.rm = TRUE)), NA),
  prd = \(x, y) gte_n(y, min_n, prd(x, y, na.rm = TRUE), NA_real_),
  prd_met = \(x, y) gte_n(y, min_n, prd_met(prd(x, y, na.rm = TRUE)), NA),
  prb = \(x, y) gte_n(y, min_n, prb(x, y, na.rm = TRUE), NA_real_),
  prb_met = \(x, y) gte_n(y, min_n, prb_met(prb(x, y, na.rm = TRUE)), NA),
  mki = \(x, y) gte_n(y, min_n, mki(x, y, na.rm = TRUE), NA_real_),
  mki_met = \(x, y) gte_n(y, min_n, mki_met(mki(x, y, na.rm = TRUE)), NA)
)



ys_fns_list <- list(
  rmse        = rmse_vec,
  r_squared   = rsq_vec,
  mae         = mae_vec,
  mpe         = mpe_vec,
  mape        = mape_vec,
  mdape       = mdape_vec # From R/helpers.R
)

compute_stats <- function(data, group_var = NULL) {
  if (!is.null(group_var)) {
    data <- data %>%
      group_by(across(all_of(group_var)))
  }

  result <- data %>%
    summarize(
      num_pin_no_class = n(),
      num_sale_no_class = sum(!is.na(sale_ratio_study_price)),
      rs_lst = list(map(rs_fns_list, ~ exec(.x, pmax(pred_pin_initial_fmv, 1), sale_ratio_study_price))),
      ys_lst = list(map(ys_fns_list, ~ exec(.x, sale_ratio_study_price, pred_pin_initial_fmv))),
      median_ratio = median(pred_pin_initial_fmv / sale_ratio_study_price, na.rm = TRUE),
      .groups = ifelse(is.null(group_var), "drop", "keep")
    ) %>%
    unnest_wider(rs_lst) %>%
    unnest_wider(ys_lst)

  return(result)
}

df_stat_no_group <- compute_stats(assessment_pin)
df_stat_triad <- compute_stats(assessment_pin, "meta_triad_code")
df_stat_township <- compute_stats(assessment_pin, "meta_township_code")
df_stat_nbhd <- compute_stats(assessment_pin, "meta_nbhd_code")

df_stat_no_group_comparison <- compute_stats(assessment_pin_comparison)
df_stat_triad_comparison <- compute_stats(assessment_pin_comparison, "meta_triad_code")
df_stat_township_comparison <- compute_stats(assessment_pin_comparison, "meta_township_code")
df_stat_nbhd_comparison <- compute_stats(assessment_pin_comparison, "meta_nbhd_code")



generate_comparison_table <- function(df_new, df_comparison, group_var = NULL) {
  if (is.null(group_var)) {
    df_comparison_renamed <- df_comparison %>%
      rename_with(~ paste0(., "_new"))

    result <- bind_cols(df_new, df_comparison_renamed)
  } else {
    df_new <- df_new %>%
      rename_with(~ paste0(., "_new"), -all_of(group_var))

    result <- inner_join(df_comparison, df_new, by = group_var)
  }

  result <- result %>%
    mutate(
      rmse_pct_diff = percentage_diff(rmse, rmse_new),
      mki_pct_diff = percentage_diff(mki, mki_new),
      cod_pct_diff = percentage_diff(cod, cod_new),
      prb_pct_diff = percentage_diff(prb, prb_new),
      prd_pct_diff = percentage_diff(prd, prd_new),
      r_squared_pct_diff = percentage_diff(r_squared, r_squared_new)
    ) %>%
    select(
      if (!is.null(group_var)) all_of(group_var) else NULL,
      rmse, rmse_new, rmse_pct_diff,
      mki, mki_new, mki_pct_diff,
      cod, cod_new, cod_pct_diff,
      prb, prb_new, prb_pct_diff,
      prd, prd_new, prd_pct_diff,
      r_squared, r_squared_new, r_squared_pct_diff
    ) %>%
    mutate(across(where(is.numeric), \(x) round(x, 2)))

  datatable(result,
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
}
```

Before running this report, make sure that the lightgbm model is set to optimize the Root Mean Square Error (RMSE). This means that the addition of a new feature *should* improve this metric. In this situation, the model sees a change of `r round(df_stat_no_group$rmse, 2)` to `r round(df_stat_no_group_comparison$rmse, 2)`, a change of `r round(df_stat_no_group$rmse - df_stat_no_group_comparison$rmse, 2)`.

Below are other features, organized by different geographies to see if the the added feature improves other metrics that the office is interested in. 


# Stats

::: {.panel-tabset}

## Overall

```{r}
generate_comparison_table(df_stat_no_group, df_stat_no_group_comparison)
```


## Triad

```{r}
generate_comparison_table(df_stat_triad, df_stat_triad_comparison, "meta_triad_code")
```

## Township

```{r}
generate_comparison_table(df_stat_township, df_stat_township_comparison, "meta_township_code")
```

## Neighborhood

```{r}
generate_comparison_table(df_stat_nbhd, df_stat_nbhd_comparison, "meta_nbhd_code")
```


## Range Test

Some metrics may only improve within certain ranges. For example, proximity to an amenity may only matter within 100 feet. The following table uses the param to filter the range to a minimum of `r params$min_range` and a maximum of `r params$max_range`.

```{r}
new_range <- assessment_pin %>%
  inner_join(assessment_data_small, by = "meta_pin") %>%
  mutate(!!params$added_feature := as.numeric(!!sym(params$added_feature))) %>%
  filter(!!sym(params$added_feature) > params$min_range &
    !!sym(params$added_feature) < params$max_range) %>%
  compute_stats(.)

comparison_range <- assessment_pin_comparison %>%
  inner_join(assessment_data_small, by = "meta_pin") %>%
  mutate(!!params$added_feature := as.numeric(!!sym(params$added_feature))) %>%
  filter(!!sym(params$added_feature) > params$min_range &
    !!sym(params$added_feature) < params$max_range) %>%
  compute_stats(.)

generate_comparison_table(new_range, comparison_range)
```


:::




# SHAP

The primary metric that the CCAO Data team uses to assess the importance of a feature is its SHAP value. SHAP values provide the amount of value each feature contributes to a parcel's predicted value. The SHAP value is calculated for each observation in the dataset, and the median SHAP value for a feature is used to determine the relative influence of that feature. The higher the median SHAP value, the more influential the feature is in the model.

## Absolute Value Rank of SHAP Scores

```{r}
shap_predictors <- unlist(metadata_new$model_predictor_all_name)
```

The following table produces the median absolute SHAP value by township, and creates a grouped table. In total, there are `r length(shap_predictors)` indicators in the model. Thus, if the median SHAP is ranked 1, it is the most important feature in a township, while if it is ranked `r length(shap_predictors)`, it is the least important feature in a township. The median value (without absolute) is also included to better contextualize the impact.


```{r shap_processing}
# Combine data
shap_df_filtered_long <- shap_new %>%
  inner_join(
    assessment_data_new %>%
      select(meta_pin, meta_card_num, meta_township_code, meta_nbhd_code) %>%
      rename(township_code = meta_township_code, neighborhood_code = meta_nbhd_code),
    by = c("meta_pin", "meta_card_num")
  ) %>%
  select(township_code, all_of(shap_predictors)) %>%
  pivot_longer(
    cols = all_of(shap_predictors),
    names_to = "feature",
    values_to = "shap"
  )
```

### SHAP Median Absolute Value
```{r shap_full_importance}
shap_df_filtered_long %>%
  group_by(feature) %>%
  mutate(
    median_abs_shap = round(median(abs(shap), na.rm = TRUE), 2),
    median_shap = round(median(shap, na.rm = TRUE), 2)
  ) %>%
  ungroup() %>%
  distinct(feature, .keep_all = TRUE) %>%
  arrange(desc(median_abs_shap)) %>%
  mutate(
    rank_absolute = row_number(),
    `Median Absolute Shap` = scales::dollar(median_abs_shap),
    `Median SHAP` = scales::dollar(median_shap)
  ) %>%
  inner_join(ccao::town_dict, by = c("township_code" = "township_code")) %>%
  ccao::vars_rename(
    names_from = "model",
    names_to = "pretty",
    type = "inplace",
    dict = ccao::vars_dict
  ) %>%
  clean_column_values("feature") %>%
  select(
    Feature = feature,
    "Median Absolute Shap",
    "Median SHAP",
    "Rank Absolute" = rank_absolute
  ) %>%
  datatable(
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
```

### SHAP Median Absolute Value by Township
```{r shap_township_importance}
shap_df_filtered_long %>%
  group_by(township_code, feature) %>%
  mutate(
    median_abs_shap = round(median(abs(shap), na.rm = TRUE), 2),
    median_shap = round(median(shap, na.rm = TRUE), 2)
  ) %>%
  ungroup() %>%
  distinct(township_code, feature, .keep_all = TRUE) %>%
  group_by(township_code) %>%
  arrange(desc(median_abs_shap), .by_group = TRUE) %>%
  mutate(
    township_rank_absolute = row_number(),
    `Median Absolute Shap` = scales::dollar(median_abs_shap),
    `Median SHAP` = scales::dollar(median_shap)
  ) %>%
  ungroup() %>%
  inner_join(ccao::town_dict, by = c("township_code" = "township_code")) %>%
  ccao::vars_rename(
    names_from = "model",
    names_to = "pretty",
    type = "inplace",
    dict = ccao::vars_dict
  ) %>%
  clean_column_values("feature") %>%
  select(
    Township = township_name,
    `Township Code` = township_code,
    Feature = feature,
    `Median Absolute Shap`,
    `Median SHAP`,
    `Township Rank Absolute` = township_rank_absolute
  ) %>%
  datatable(
    options = list(
      scrollY = "300px",
      scrollX = TRUE,
      paging = FALSE,
      searching = TRUE
    ),
    rownames = FALSE
  )
```


```{r violin_plots_shap_to_feature}
# Calculate the number of digits
num_digits <- card_individual %>%
  pull({{ target_feature_shap }}) %>%
  max(na.rm = TRUE) %>%
  floor() %>%
  as.character() %>%
  str_length()

quantiles <- card_individual %>%
  pull({{ target_feature_shap }}) %>%
  quantile(c(0.025, 0.975), na.rm = TRUE)

# Create the violin plot, excluding outliers only in the display
card_individual %>%
  select(meta_card_num, meta_pin, {{ target_feature_shap }}, {{ target_feature_value }}) %>%
  mutate(bin = cut_number(!!sym(target_feature_value), n = 10, dig.lab = num_digits)) %>%
  ggplot(aes(x = bin, y = !!sym(target_feature_shap))) +
  geom_violin(fill = "#69b3a2") +
  theme_minimal() +
  xlab("Feature Value") +
  ylab("SHAP Value") +
  scale_x_discrete(labels = function(x) {
    x <- gsub("\\.[^,\\]]*", "", x) # Clean the factor levels for chart
    x <- gsub("[^0-9,,]", "", x)
    gsub(",", "-", x)
  }) +
  coord_cartesian(ylim = quantiles) + # Focus only on the range between the 2.5% and 97.5% quantiles
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

### Scatterplot Demonstrating the Relationship between SHAP and Feature Value

```{r}
shapviz::shapviz(
  object = shap_new %>%
    select(all_of(shap_predictors)) %>%
    as.matrix(),
  X = assessment_card_new %>%
    select(all_of(shap_predictors)),
  baseline = shap_new$pred_card_shap_baseline_fmv[1]
) %>%
  shapviz::sv_dependence(
    v = target_feature_value
  )
```

# Spatial Analysis

This panel looks at four stats, aggregated on the neighborhood level; the mean of the added feature, the median of the added feature, the mean of the absolute value of the SHAP, the 90th percentile of the SHAP, and the change in FMV based on the added feature.

## Neighborhood Values

::: panel-tabset

### Mean Value of the Feature by Neighborhood

```{r mean_feature}
pin_nbhd %>%
  ggplot() +
  geom_sf(aes(fill = !!sym(paste0({{ target_feature_value }}, "_neighborhood_mean")))) +
  scale_fill_viridis_c(option = "viridis", name = "Value") +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```

### Median Value of the Feature by Neighborhood

```{r mean_feature}
pin_nbhd %>%
  ggplot() +
  geom_sf(aes(fill = !!sym(paste0({{ target_feature_value }}, "_neighborhood_median")))) +
  scale_fill_viridis_c(option = "viridis", name = "Value") +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```

### Mean Absolute SHAP Value

```{r mean_shap}
card_nbhd %>%
  ggplot() +
  geom_sf(aes(fill = !!sym(paste0({{ target_feature_shap }}, "_mean_abs")))) +
  scale_fill_viridis_c(option = "viridis", name = "Value") +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```

### 90th Percentile of Absolute SHAP

```{r 90th_percentile_shap}
card_nbhd %>%
  ggplot() +
  geom_sf(aes(fill = !!sym(paste0({{ target_feature_shap }}, "_90th")))) +
  scale_fill_viridis_c(option = "viridis", name = "Value") +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```

### Median change in Neighborhood FMV
This value is defined as the neighborhood level median increase in value when adding the new feature to the model. For example, a value of 1% would mean that adding the feature increased properties within a neighborhood by 1%.

```{r neighborhood_change}
assessment_pin_new %>%
  select(meta_pin, loc_longitude, loc_latitude, pred_pin_final_fmv) %>%
  rename(pred_pin_final_fmv_new = pred_pin_final_fmv) %>%
  inner_join(assessment_pin_comparison, by = "meta_pin") %>%
  rename(pred_pin_final_fmv_comparison = pred_pin_final_fmv) %>%
  group_by(meta_nbhd_code) %>%
  summarize(
    median_fmv_new = median(pred_pin_final_fmv_new, na.rm = TRUE),
    median_fmv_comparison = median(pred_pin_final_fmv_comparison, na.rm = TRUE),
    fmv_ratio = (median_fmv_new / median_fmv_comparison) / median_fmv_comparison
  ) %>%
  inner_join(nbhd, by = c("meta_nbhd_code" = "town_nbhd")) %>%
  st_as_sf() %>%
  ggplot() +
  geom_sf(aes(fill = fmv_ratio)) +
  scale_fill_viridis_c(option = "viridis", name = "FMV Ratio", labels = scales::percent_format(accuracy = 0.001)) +
  theme_void() +
  coord_sf(xlim = c(-88.4, -87.52398), ylim = c(41.5, 42.2))
```

:::
```{r}
create_leaflet_map <- function(dataset, legend_value, legend_title, order_scheme = "high", longitude = "loc_longitude", latitude = "loc_latitude") {
  # Filter neighborhoods that have at least one observation
  nbhd_borders <- nbhd %>%
    right_join(dataset, by = c("town_nbhd" = "meta_nbhd_code"))

  # Create the color palette based on order_scheme
  if (order_scheme == "low") {
    pal <- colorNumeric(palette = "Reds", domain = dataset[[legend_value]], reverse = TRUE)
  } else {
    pal <- colorNumeric(palette = "Reds", domain = dataset[[legend_value]])
  }

  # Calculate the bounding box of the filtered neighborhoods
  bbox <- st_bbox(nbhd_borders)

  # Create the leaflet map
  leaflet(dataset) %>%
    addProviderTiles(providers$CartoDB.Positron) %>%
    addCircleMarkers(
      lng = ~ get(longitude),
      lat = ~ get(latitude),
      radius = 5,
      color = ~ pal(dataset[[legend_value]]),
      popup = ~ {
        shap_values <- dataset %>%
          select(starts_with("target_feature_shap_")) %>%
          summarise_all(~ ifelse(!is.na(.), sprintf("SHAP: %s", scales::dollar(.)), NA)) %>%
          apply(1, function(row) {
            paste(na.omit(row), collapse = "<br>")
          })
        paste(
          "Pin: ", meta_pin,
          ifelse(shap_values == "", "", paste0("<br>", shap_values)),
          "<br>", "Relative SHAP: ", scales::percent(relative_shap, accuracy = 0.01),
          "<br>", "Feature: ", sprintf("%.2f", get(params$added_feature)),
          "<br>", "New FMV: ", pred_pin_final_fmv_new,
          "<br>", "Comparison FMV: ", pred_pin_final_fmv_comp,
          "<br>", "FMV Difference: ", scales::percent(diff_pred_pin_final_fmv)
        )
      }
    ) %>%
    addPolygons(
      data = nbhd_borders,
      color = "black",
      weight = 2,
      fill = FALSE
    ) %>%
    addLegend(
      "bottomright",
      pal = pal,
      values = dataset[[legend_value]],
      title = legend_title
    )
}
```

## Highest and Lowest 100 Values

Three different types of high and low values are produced; the values of the feature we are analyzing, the impact that can be determined through the SHAPs, and the largest effects in change in FMV.

::: panel-tabset
### Largest 100 Values

Be careful interpreting values which are the max and min of the raw value, since ties are not accounted for. For example, if there are 10,000 parcels which are 0 feet from a newly constructed building, the map will not be a full representation.

```{r}
highest_100 <- leaflet_data %>%
  arrange(desc(!!sym(target_feature_value))) %>%
  dplyr::slice(1:100)

create_leaflet_map(highest_100, {{ target_feature_value }}, "Largest 100 Values")
```

### Lowest 100 Values

Be careful interpreting values which are the max and min of the raw value, since ties are not accounted for. For example, if there are 10,000 parcels which are 0 feet from a newly constructed building, the map will not be a full representation.

```{r}
lowest_100 <- leaflet_data %>%
  distinct(meta_pin, .keep_all = TRUE) %>%
  arrange(!!sym({{ target_feature_value }})) %>%
  slice(1:100)

create_leaflet_map(lowest_100, {{ target_feature_value }}, "Lowest 100 Values", order_scheme = "low")
```

### Highest 100 SHAP Values

```{r}
highest_100 <- leaflet_data %>%
  arrange(desc(shap_total)) %>%
  slice(1:100)

create_leaflet_map(highest_100, "shap_total", "Highest 100 SHAPs")
```

### Lowest 100 SHAP Values

```{r}
lowest_100 <- leaflet_data %>%
  arrange(shap_total) %>%
  slice(1:100)

create_leaflet_map(lowest_100, "shap_total", "Lowest 100 SHAPs", order_scheme = "low")
```
:::

## Largest FMV Changes

Multicard parcels have heuristic which limits their change. The added feature may trigger (or not trigger it), leading to changes much larger than the added feature's impact.

::: panel-tabset
### 100 Largest FMV Increases

```{r}
largest_fmv_increases <- leaflet_data %>%
  arrange(desc(diff_pred_pin_final_fmv)) %>%
  slice(1:100)

# Call the function with the pre-sliced dataset
create_leaflet_map(largest_fmv_increases, "diff_pred_pin_final_fmv", "Largest FMV Increases (%)")
```

### 100 Largest FMV Decreases

Multicard parcels have heuristic which limits their change. The added feature may trigger (or not trigger it), leading to changes much larger than the added feature's impact.

```{r}
largest_fmv_decreases <- leaflet_data %>%
  arrange(diff_pred_pin_final_fmv) %>%
  slice(1:100)

create_leaflet_map(largest_fmv_decreases, "diff_pred_pin_final_fmv", "Largest FMV Decreases (%)", order_scheme = "low")
```

### 100 Largest FMV Initial Increases

```{r}
largest_fmv_increases <- leaflet_data %>%
  arrange(desc(diff_pred_pin_initial_fmv)) %>%
  slice(1:100)

# Call the function with the pre-sliced dataset
create_leaflet_map(largest_fmv_increases, "diff_pred_pin_initial_fmv", "Largest FMV Increases (%)")
```

### 100 Largest Initial FMV Decreases

```{r}
largest_fmv_decreases <- leaflet_data %>%
  arrange(diff_pred_pin_initial_fmv) %>%
  slice(1:100)

create_leaflet_map(largest_fmv_decreases, "diff_pred_pin_initial_fmv", "Largest FMV Decreases (%)", order_scheme = "low")
```

## Largest FMV Increases no Multicards

```{r}
largest_fmv_increases <- leaflet_data %>%
  group_by(meta_pin) %>%
  filter(n() == 1) %>%
  ungroup() %>%
  arrange(desc(diff_pred_pin_final_fmv)) %>%
  slice(1:100)

create_leaflet_map(largest_fmv_increases, "diff_pred_pin_final_fmv", "Largest FMV Increases")
```

## Largest FMV Decreases no Multicards

```{r}
largest_fmv_decreases <- leaflet_data %>%
  group_by(meta_pin) %>%
  filter(n() == 1) %>%
  ungroup() %>%
  arrange(diff_pred_pin_initial_fmv) %>%
  slice(1:100)

create_leaflet_map(largest_fmv_increases, "diff_pred_pin_final_fmv", "Largest FMV Increases (%)", order_scheme = "low")
```
:::

## Neighborhoods with the Highest and Lowest SHAP Values

These maps identify neighborhoods where the added feature is having the largest impact on SHAP values. By selecting neighborhoods with the highest mean(absolute value), you can take a closer look at how individual parcels in these neighborhoods are affected.

::: panel-tabset
```{r processing_SHAP_values}
selected_data <- leaflet_data %>%
  group_by(meta_nbhd_code) %>%
  mutate(mean_value = mean(abs(shap_total)), na.rm = TRUE) %>%
  ungroup() %>%
  distinct(meta_nbhd_code, .keep_all = TRUE) %>%
  arrange(mean_value)

# Select top 2 and bottom 2 neighborhoods based on mean SHAP values
selected_nbhd_codes <- selected_data %>%
  slice(c(1:2, (n() - 1):n())) %>%
  pull(meta_nbhd_code)


filtered_data <- filter(leaflet_data, meta_nbhd_code %in% selected_nbhd_codes)


# Separate high and low mean value neighborhoods
high_mean_data <- filtered_data %>%
  filter(meta_nbhd_code %in% selected_nbhd_codes[(length(selected_nbhd_codes) - 1):length(selected_nbhd_codes)])

low_mean_data <- filtered_data %>%
  filter(meta_nbhd_code %in% selected_nbhd_codes[1:2])
```

### 2 Highest SHAP Neighborhoods

```{r}
create_leaflet_map(high_mean_data, "shap_total", "SHAP Values")
```

### 2 Lowest SHAP Neighborhoods

```{r}
create_leaflet_map(low_mean_data, "shap_total", "SHAP Values")
```
:::
