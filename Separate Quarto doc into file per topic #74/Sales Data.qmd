---
title: "Model performance for `r params$run_id`"
execute:
  echo: false
  warning: false
format:
  html:
    embed-resources: true
    toc: true
    toc_float: true
    fig-align: center
    fontsize: 12pt
editor: source
params:
  run_id: 2023-03-14-clever-damani
  year: '2023'
---

```{r setup}
options(knitr.kable.NA = "", scipen = 99, width = 150)
noctua::noctua_options(cache_size = 10)

# Load necessary libraries
library(arrow)
library(ccao)
library(brio)
library(DBI)
library(decor)
library(desc)
library(dplyr)
library(DT)
library(ggplot2)
library(glue)
library(grid)
library(gridExtra)
library(gtools)
library(here)
library(htmltools)
library(kableExtra)
library(leaflet)
library(noctua)
library(plotly)
library(purrr)
library(recipes)
library(scales)
library(sf)
library(skimr)
library(stringr)
library(tableone)
library(tidyr)
library(tools)
source(here("R", "helpers.R"))

# TODO: Catch for weird arrow bug with SIGPIPE. Need to permanently fix later
# https://github.com/apache/arrow/issues/32026
cpp11::cpp_source(code = "
#include <csignal>
#include <cpp11.hpp>

[[cpp11::register]] void ignore_sigpipes() {
  signal(SIGPIPE, SIG_IGN);
}
")

ignore_sigpipes()

# Initialize a dictionary of file paths. See misc/file_dict.csv for details
paths <- model_file_dict(params$run_id, params$year)

# Grab metadata to check input alignment
metadata <- read_parquet(paths$output$metadata$local)
if (metadata$run_id != params$run_id) {
  stop(
    "Local run outputs are NOT equal to the requested run_id. You ",
    "should run model_fetch_run() to fetch model outputs from S3"
  )
}

# Get the triad of the run to use for filtering
run_triad <- tools::toTitleCase(metadata$assessment_triad)
run_triad_code <- ccao::town_dict %>%
  filter(triad_name == run_triad) %>%
  distinct(triad_code) %>%
  pull(triad_code)

# Ingest training set used for this run from DVC bucket (if not local)
training_data <- read_parquet(paths$input$training$local)

# Load model-generated output data sets
assessment_card <- read_parquet(paths$output$assessment_card$local)
assessment_pin <- read_parquet(paths$output$assessment_pin$local)

# Load SHAP and feature importance data
shap_exists <- file.exists(paths$output$shap$local)
if (shap_exists) {
  shap_df <- read_parquet(paths$output$shap$local)
}
feat_imp_df <- read_parquet(paths$output$feature_importance$local)
```


## Sales Data

::: panel-tabset
## By Year

*NOTE: Outliers are removed*

```{r town_year}
township <- training_data %>%
  filter(!sv_is_outlier, meta_triad_name == run_triad) %>%
  mutate(meta_year = as.numeric(meta_year)) %>%
  group_by(meta_township_name, meta_year) %>%
  summarise(sales = n()) %>%
  select(
    Sales = sales,
    Year = meta_year,
    Township = meta_township_name
  ) %>%
  ggplot() +
  geom_line(aes(x = Year, y = Sales, color = Township)) +
  scale_y_continuous(labels = scales::comma, breaks = seq(0, 6000, 1000)) +
  scale_x_continuous(breaks = seq(
    (as.numeric(params$year) - 9),
    as.numeric(params$year), 1
  )) +
  theme_minimal() +
  theme(axis.title.x = element_blank())

ggplotly(township)
```

## By Quarter

Sales by bin by by quarter for the previous 4 years. The goal is to see how different segments of the market are changing as a proportion in each tri-town.

*NOTE: Outliers are removed*

```{r town_bin1, fig.height=8, fig.width=7, out.width="100%"}
training_data %>%
  filter(
    !sv_is_outlier,
    meta_triad_name == run_triad,
    meta_year >= as.numeric(params$year) - 4
  ) %>%
  mutate(
    Bin = cut(
      meta_sale_price,
      breaks = c(1, 100000, 300000, 600000, 1000000, max(meta_sale_price)),
      labels = c(
        "$1 - $100,000",
        "$100,000 - $300,000",
        "$300,000 - $600,000",
        "$600,000 - $1,000,000",
        "$1,000,000+"
      )
    ),
    Quarter = lubridate::quarter(meta_sale_date) + 4 *
      (as.numeric(meta_year) - as.numeric(params$year))
  ) %>%
  group_by(meta_township_name, Quarter, Bin) %>%
  summarise(Sales = n()) %>%
  ungroup() %>%
  select(Sales, Bin, Township = meta_township_name, Quarter) %>%
  ggplot(aes(
    x = Quarter,
    y = Sales,
    fill = Bin,
    group = Bin
  )) +
  geom_area() +
  scale_color_brewer(palette = "PuOr") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 90, hjust = 1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  facet_wrap(vars(Township), scales = "free_y", ncol = 3)
```

## By Class and Year

*NOTE: Outliers are removed*

```{r class_year}
class <- training_data %>%
  filter(
    !sv_is_outlier,
    meta_triad_name == run_triad,
    !ind_pin_is_multicard
  ) %>%
  mutate(meta_year = as.numeric(meta_year)) %>%
  group_by(meta_class, meta_year) %>%
  summarise(sales = n()) %>%
  rename(Class = meta_class) %>%
  pivot_wider(
    id_cols = Class,
    names_from = meta_year,
    values_from = sales
  )

class %>%
  kable() %>%
  kable_styling() %>%
  row_spec(unique(which(is.na(class), arr.ind = TRUE)[, 1]),
    bold = TRUE,
    background = "lightgrey"
  )
```

## Removed Outliers

These are outliers that were removed via the sale validation process in the ingest stage. The goal is to confirm that these were reasonable sales to remove.

```{r}
rbind(
  training_data %>%
    filter(
      sv_is_outlier,
      meta_triad_name == run_triad,
      !ind_pin_is_multicard
    ) %>%
    slice_max(order_by = meta_sale_price, n = 25),
  training_data %>%
    filter(
      sv_is_outlier,
      meta_triad_name == run_triad,
      !ind_pin_is_multicard
    ) %>%
    slice_min(order_by = meta_sale_price, n = 25)
) %>%
  datatable(rownames = FALSE)
```

## Kept Outliers

These are the top and bottom 25 sales from the training data that are *still* in the sample. The goal is to gauge whether or not outlier removal is being too strict (or not strict enough).

```{r}
rbind(
  training_data %>%
    filter(
      !sv_is_outlier,
      meta_triad_name == run_triad,
      !ind_pin_is_multicard
    ) %>%
    slice_max(order_by = meta_sale_price, n = 25),
  training_data %>%
    filter(
      !sv_is_outlier,
      meta_triad_name == run_triad,
      !ind_pin_is_multicard
    ) %>%
    slice_min(order_by = meta_sale_price, n = 25)
) %>%
  datatable(rownames = FALSE)
```
:::



