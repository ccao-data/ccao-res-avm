---
title: "Model performance for `r params$run_id`"
execute:
  echo: false
  warning: false
format:
  html:
    embed-resources: true
    toc: true
    toc_float: true
    fig-align: center
    fontsize: 12pt
editor: source
params:
  run_id: 2023-03-14-clever-damani
  year: '2023'
  dvc_bucket: 's3://ccao-data-dvc-us-east-1'
---


```{r setup}
options(knitr.kable.NA = "", scipen = 99, width = 150)

# Load necessary libraries
library(arrow)
library(dplyr)
library(DT)
library(ggplot2)
library(glue)
library(here)
library(htmltools)
library(kableExtra)
library(leaflet)
library(plotly)
library(purrr)
library(recipes)
library(scales)
library(sf)
library(skimr)
library(stringr)
library(tableone)
library(tidyr)
source(here("R", "helpers.R"))

# TODO: Catch for weird arrow bug with SIGPIPE. Need to permanently fix later
# https://github.com/apache/arrow/issues/32026
cpp11::cpp_source(code = "
#include <csignal>
#include <cpp11.hpp>

[[cpp11::register]] void ignore_sigpipes() {
  signal(SIGPIPE, SIG_IGN);
}
")

ignore_sigpipes()

# Initialize a dictionary of file paths. See misc/file_dict.csv for details
paths <- model_file_dict(params$run_id, params$year)

# Grab metadata to check input alignment
metadata <- read_parquet(paths$output$metadata$local)
if (metadata$run_id != params$run_id) {
  stop(
    "Local run outputs are NOT equal to the requested run_id. You ",
    "should run model_fetch_run() to fetch model outputs from S3"
  )
}

# Get the triad of the run to use for filtering
run_triad <- tools::toTitleCase(metadata$assessment_triad)
run_triad_code <- ccao::town_dict %>%
  filter(triad_name == run_triad) %>%
  distinct(triad_code) %>%
  pull(triad_code)

# Ingest training set used for this run from DVC bucket (if not local)
train_md5_s3 <- metadata$dvc_md5_training_data
train_md5_local <- digest::digest(paths$input$training$local, algo = "md5")
train_md5 <- if (train_md5_s3 != train_md5_local) train_md5_s3 else train_md5_local
training_path <- paste0(
  params$dvc_bucket, "/",
  substr(train_md5, 1, 2), "/",
  substr(train_md5, 3, nchar(train_md5))
)
training_data <- read_parquet(training_path)

# Load model-generated output data sets
assessment_card <- read_parquet(paths$output$assessment_card$local)
assessment_pin <- read_parquet(paths$output$assessment_pin$local)

# Load SHAP and feature importance data
shap_exists <- file.exists(paths$output$shap$local)
if (shap_exists) {
  shap_df <- read_parquet(paths$output$shap$local)
}
feat_imp_df <- read_parquet(paths$output$feature_importance$local)
```

## Sales Data

::: {.panel-tabset}

## By Year

*NOTE: Outliers are removed*

``` {r town_year}
township <- training_data %>%
  filter(!sv_is_outlier, meta_triad_name == run_triad) %>%
  mutate(meta_year = as.numeric(meta_year)) %>%
  group_by(meta_township_name, meta_year) %>%
  summarise(sales = n()) %>%
  select(Sales = sales, Year = meta_year, Township = meta_township_name) %>%
  ggplot() +
  geom_line(aes(x = Year, y = Sales, color = Township)) +
  scale_y_continuous(labels = scales::comma, breaks = seq(0, 6000, 1000)) +
  scale_x_continuous(breaks = seq(2014, 2022, 1)) +
  theme_minimal() +
  theme(axis.title.x = element_blank())

ggplotly(township)
```

## By Quarter

Sales by bin by quarter since 2018. Goal is to see how different segments of the market are changing as a proportion in each tri-town.

*NOTE: Outliers are removed*

``` {r town_bin1, fig.height=8, fig.width=7, out.width="100%"}
training_data %>%
  filter(!sv_is_outlier, meta_triad_name == run_triad, meta_year >= "2018") %>%
  mutate(
    Bin = cut(
      meta_sale_price,
      breaks = c(1, 100000, 300000, 600000, 1000000, max(meta_sale_price)),
      labels = c(
        "$1 - $100,000",
        "$100,000 - $300,000",
        "$300,000 - $600,000",
        "$600,000 - $1,000,000",
        "$1,000,000+"
      )
    ),
    Quarter = lubridate::quarter(meta_sale_date) + 4 *
      (as.numeric(meta_year) - 2018)
  ) %>%
  group_by(meta_township_name, Quarter, Bin) %>%
  summarise(Sales = n()) %>%
  ungroup() %>%
  select(Sales, Bin, Township = meta_township_name, Quarter) %>%
  ggplot(aes(x = Quarter, y = Sales, fill = Bin, group = Bin)) +
  geom_area() +
  scale_color_brewer(palette = "PuOr") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 90, hjust = 1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) +
  facet_wrap(vars(Township), scales = "free_y", ncol = 3)
```

## By Class and Year

*NOTE: Outliers are removed*

``` {r class_year}
class <- training_data %>%
  filter(
    !sv_is_outlier,
    meta_triad_name == run_triad,
    !ind_pin_is_multicard
  ) %>%
  mutate(meta_year = as.numeric(meta_year)) %>%
  group_by(meta_class, meta_year) %>%
  summarise(sales = n()) %>%
  rename(Class = meta_class) %>%
  pivot_wider(id_cols = Class, names_from = meta_year, values_from = sales)

class %>%
  kable() %>%
  kable_styling() %>%
  row_spec(
    unique(which(is.na(class), arr.ind = TRUE)[, 1]),
    bold = TRUE,
    background = "lightgrey"
  )
```

## Removed Outliers

These are outliers that were removed via the sale validation process in the ingest stage. The goal is to confirm that these were reasonable sales to remove.

``` {r}
rbind(
  training_data %>%
    filter(
      sv_is_outlier,
      meta_triad_name == run_triad,
      !ind_pin_is_multicard
    ) %>%
    slice_max(order_by = meta_sale_price, n = 25),
  training_data %>%
    filter(
      sv_is_outlier,
      meta_triad_name == run_triad,
      !ind_pin_is_multicard
    ) %>%
    slice_min(order_by = meta_sale_price, n = 25)
) %>%
  datatable(rownames = FALSE)
```

## Kept Outliers

These are the top and bottom 25 sales from the training data that are *still* in the sample. The goal is to gauge whether or not outlier removal is being too strict (or not strict enough).

``` {r}
rbind(
  training_data %>%
    filter(
      !sv_is_outlier,
      meta_triad_name == run_triad,
      !ind_pin_is_multicard
    ) %>%
    slice_max(order_by = meta_sale_price, n = 25),
  training_data %>%
    filter(
      !sv_is_outlier,
      meta_triad_name == run_triad,
      !ind_pin_is_multicard
    ) %>%
    slice_min(order_by = meta_sale_price, n = 25)
) %>%
  datatable(rownames = FALSE)
```

:::

## Model Time Tracking

::: {.panel-tabset}

## Training Data (Seen)

Does the model accurately model time trends for the data it's already seen?

```{r, fig.height=8, fig.width=7, out.width="100%"}
model_fit <- lightsnip::lgbm_load(paths$output$workflow_fit$local)
model_recipe <- readRDS(paths$output$workflow_recipe$local)

training_data_pred <- training_data %>%
  mutate(
    pred_card_initial_fmv = predict(
      model_fit,
      new_data = bake(model_recipe, new_data = ., all_predictors())
    )$.pred
  )

training_data_pred %>%
  filter(
    !sv_is_outlier,
    meta_triad_name == run_triad,
    !ind_pin_is_multicard
  ) %>%
  mutate(
    time_interval = lubridate::interval(
      lubridate::make_date(metadata$input_min_sale_year, 1, 1),
      lubridate::ymd(meta_sale_date)
    ),
    time_sale_month = as.numeric(time_interval %/% lubridate::dmonths(1)) + 1
  ) %>%
  group_by(meta_township_name, time_sale_month) %>%
  summarize(
    `Median Prediction` = median(pred_card_initial_fmv),
    `Median Sale Price` = median(meta_sale_price)
  ) %>%
  tidyr::pivot_longer(cols = starts_with("Median")) %>%
  ggplot() +
  geom_line(aes(x = time_sale_month, y = value, color = name)) +
  scale_color_manual(
    name = "",
    values = c("Median Prediction" = "red", "Median Sale Price" = "blue")
  ) +
  scale_y_continuous(labels = scales::label_dollar(scale = 1e-3, suffix = "K")) +
  facet_wrap(vars(meta_township_name), scales = "free_y", ncol = 3) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 90, hjust = 1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )
```

## Test Data (Unseen)

Does the model accurately model time trends for unseen data slightly in the future?

```{r, fig.height=8, fig.width=7, out.width="100%"}
test_data <- read_parquet(paths$output$test_card$local)
test_data %>%
  filter(meta_triad_code == run_triad_code) %>%
  mutate(
    time_interval = lubridate::interval(
      lubridate::make_date(metadata$input_min_sale_year, 1, 1),
      lubridate::ymd(meta_sale_date)
    ),
    time_sale_month = as.numeric(time_interval %/% lubridate::dmonths(1)) + 1,
    meta_township_name = ccao::town_convert(meta_township_code)
  ) %>%
  group_by(meta_township_name, time_sale_month) %>%
  summarize(
    `Median Prediction` = median(pred_card_initial_fmv),
    `Median Sale Price` = median(meta_sale_price)
  ) %>%
  tidyr::pivot_longer(cols = starts_with("Median")) %>%
  ggplot() +
  geom_line(aes(x = time_sale_month, y = value, color = name)) +
  scale_color_manual(
    name = "",
    values = c("Median Prediction" = "red", "Median Sale Price" = "blue")
  ) +
  scale_y_continuous(labels = scales::label_dollar(scale = 1e-3, suffix = "K")) +
  facet_wrap(vars(meta_township_name), scales = "free_y", ncol = 3) +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    axis.text.x = element_text(angle = 90, hjust = 1),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  )
```

:::

```{r, results='asis', eval=shap_exists}
cat("## SHAP Values")
cat("::: {.panel-tabset}")
```

```{r, results='asis', eval=shap_exists}
cat("## SHAP Summary")
```

```{r, eval=shap_exists}
shap_df_filtered <- shap_df %>%
  left_join(
    assessment_card %>%
      mutate(meta_triad = ccao::town_get_triad(meta_township_code)) %>%
      select(meta_year, meta_pin, meta_card_num, meta_triad),
    by = c("meta_year", "meta_pin", "meta_card_num")
  ) %>%
  filter(meta_triad == run_triad_code) %>%
  arrange(meta_pin, meta_card_num)
```

```{r, eval=shap_exists}
shap_df_filtered %>%
  select(
    where(~ !all(is.na(.x))) & !starts_with("pred") & where(is.numeric)
  ) %>%
  skim() %>%
  rename_with(~ str_replace_all(.x, "skim_|numeric.", "")) %>%
  select(-c(type, n_missing, complete_rate, p25, p75)) %>%
  rename(histogram = hist, median = p50, min = p0, max = p100) %>%
  mutate(
    variable = str_replace_all(variable, "_", " "),
    across(where(is.numeric), ~ round(.x, 1)),
    `mean direction` = case_when(
      mean < 0 ~ "-", mean > 0 ~ "+", TRUE ~ ""
    ),
    `median direction` = case_when(
      median < 0 ~ "-", median > 0 ~ "+", TRUE ~ ""
    ),
    mean = abs(mean),
    median = abs(median)
  ) %>%
  relocate(`mean direction`, .before = mean) %>%
  relocate(`median direction`, .before = median) %>%
  datatable(
    rownames = FALSE,
    height = "500px",
    options = list(
      columnDefs = list(
        list(className = "dt-center", targets = c(1:7))
      )
    )
  )
```

```{r, eval=shap_exists}
shap_predictors <- names(shap_df_filtered)
shap_predictors <- shap_predictors[
  !shap_predictors %in% c(
    "meta_year", "meta_pin", "meta_card_num",
    "pred_card_shap_baseline_fmv", "township_code", "meta_triad"
  )
]

assessment_card_filtered <- assessment_card %>%
  mutate(meta_triad = ccao::town_get_triad(meta_township_code)) %>%
  filter(meta_triad == run_triad_code) %>%
  arrange(meta_pin, meta_card_num)

create_shapviz <- function(shap_df, assessment_df, idx) {
  shap_predictors <- names(shap_df)
  shap_predictors <- shap_predictors[
    !shap_predictors %in% c(
      "meta_year", "meta_pin", "meta_card_num",
      "pred_card_shap_baseline_fmv", "township_code", "meta_triad"
    )
  ]

  shapviz::shapviz(
    object = shap_df %>%
      select(all_of(shap_predictors)) %>%
      slice(idx) %>%
      as.matrix(),
    X = assessment_df %>%
      select(all_of(shap_predictors)) %>%
      slice(idx),
    baseline = shap_df$pred_card_shap_baseline_fmv[1]
  )
}
```

```{r, results='asis', eval=shap_exists}
cat("## Riverside Beeswarm")
```

```{r, eval=shap_exists}
shap_riverside_idx <- which(
  assessment_card_filtered$meta_township_code == ccao::town_convert("Riverside")
)
create_shapviz(shap_df_filtered, assessment_card_filtered, shap_riverside_idx) %>%
  shapviz::sv_importance("beeswarm") +
  theme_minimal()
```

```{r, results='asis', eval=shap_exists}
cat("## Calumet Beeswarm")
```

```{r, eval=shap_exists}
shap_calumet_idx <- which(
  assessment_card_filtered$meta_township_code == ccao::town_convert("Calumet")
)
create_shapviz(shap_df_filtered, assessment_card_filtered, shap_calumet_idx) %>%
  shapviz::sv_importance("beeswarm") +
  theme_minimal()
```

```{r, results='asis', eval=shap_exists}
cat("## Thornton Beeswarm")
```

```{r, eval=shap_exists}
shap_thornton_idx <- which(
  assessment_card_filtered$meta_township_code == ccao::town_convert("Thornton")
)
create_shapviz(shap_df_filtered, assessment_card_filtered, shap_thornton_idx) %>%
  shapviz::sv_importance("beeswarm") +
  theme_minimal()
```

```{r, results='asis', eval=shap_exists}
cat(":::")
```

## Township Outliers and Accuracy

::: {.panel-tabset}

## Big Misses

What is the 2022 sale with the biggest miss for each town and price quartile?

```{r}
qnt_df <- assessment_pin %>%
  mutate(Town = ccao::town_convert(meta_township_code)) %>%
  filter(
    Town %in% c("Calumet", "Thornton", "Bloom"),
    !is.na(sale_recent_1_price),
    sale_recent_1_date >= lubridate::make_date(2022)
  ) %>%
  mutate(pred_pin_final_fmv_round) %>%
  select(
    PIN = meta_pin, Town, Class = meta_class,
    NBHD = meta_nbhd_code, `Bldg Sqft` = char_total_bldg_sf, Yrblt = char_yrblt,
    `Sale 1 Date` = sale_recent_2_date, `Sale 1 Price` = sale_recent_2_price,
    `Sale 2 Date` = sale_recent_1_date, `Sale 2 Price` = sale_recent_1_price,
    `Est. FMV` = pred_pin_final_fmv_round
  ) %>%
  group_by(Town) %>%
  mutate(
    diff = abs(`Sale 2 Price` - `Est. FMV`),
    `Qnt.` = cut(
      `Sale 2 Price`,
      breaks = quantile(`Sale 2 Price`, probs = c(0, 0.25, 0.5, 0.75, 1)),
      labels = c("Q1", "Q2", "Q3", "Q4"),
      include.lowest = TRUE
    )
  ) %>%
  group_by(Town, `Qnt.`) %>%
  slice_max(diff, n = 1) %>%
  select(-diff) %>%
  relocate(`Qnt.`, .after = "Town")

qnt_df %>%
  datatable(rownames = FALSE)
```

## MAPE by Quintile

How does accuracy as measured by MAPE (Mean Absolute Percentage Error) change by quintile for each tri-town?
  
``` {r}
mape <- training_data_pred %>%
  filter(!sv_is_outlier) %>%
  left_join(
    ccao::town_dict %>%
      select(meta_township_code = township_code, triad_name)
  ) %>%
  filter(triad_name == run_triad, meta_year == max(meta_year)) %>%
  group_by(meta_township_name) %>%
  arrange(meta_sale_price) %>%
  mutate(Quintile = ntile(meta_sale_price, 5)) %>%
  group_by(meta_township_name, Quintile) %>%
  summarise(
    median_sale_price = median(meta_sale_price, na.rm = TRUE),
    median_predicted_fmv = median(pred_card_initial_fmv, na.rm = TRUE),
    MAPE = yardstick::mape_vec(meta_sale_price, pred_card_initial_fmv)
  ) %>%
  rename(Township = meta_township_name)

ggplotly(
  mape %>%
    ggplot(aes(x = Quintile, y = MAPE, color = Township)) +
    geom_line() +
    theme_minimal()
)
```

## Predicted v Actual By Town

```{r}
pred_v_actual_town <- test_data %>%
  filter(meta_triad_code == run_triad_code) %>%
  mutate(Township = ccao::town_convert(meta_township_code)) %>%
  rename(
    `Sale Price` = meta_sale_price,
    `Predicted FMV` = pred_card_initial_fmv,
    PIN = meta_pin
  ) %>%
  ggplot() +
  geom_point(aes(
    group = PIN,
    x = `Predicted FMV`,
    y = `Sale Price`,
    color = Township
  )) +
  geom_abline(slope = 1, intercept = 0) +
  scale_x_continuous(
    name = "Predicted FMV",
    labels = scales::label_dollar(
      accuracy = 1,
      scale = 1 / 1000,
      suffix = "K"
    ),
    n.breaks = 5,
    limits = c(1e4, 1.5e6)
  ) +
  scale_y_continuous(
    name = "Actual FMV",
    labels = scales::label_dollar(
      accuracy = 1,
      scale = 1 / 1000,
      suffix = "K"
    ),
    n.breaks = 5,
    limits = c(1e4, 1.5e6)
  ) +
  theme_minimal()

ggplotly(pred_v_actual_town)
```

## Predicted v Actual By Class

```{r}
pred_v_actual_class <- test_data %>%
  filter(meta_triad_code == run_triad_code) %>%
  mutate(Township = ccao::town_convert(meta_township_code)) %>%
  rename(
    `Sale Price` = meta_sale_price,
    `Predicted FMV` = pred_card_initial_fmv,
    PIN = meta_pin,
    Class = meta_class
  ) %>%
  ggplot() +
  geom_point(aes(
    group = PIN,
    x = `Predicted FMV`,
    y = `Sale Price`,
    color = Class
  )) +
  geom_abline(slope = 1, intercept = 0) +
  scale_x_continuous(
    name = "Predicted FMV",
    labels = scales::label_dollar(
      accuracy = 1,
      scale = 1 / 1000,
      suffix = "K"
    ),
    n.breaks = 5,
    limits = c(1e4, 1.5e6)
  ) +
  scale_y_continuous(
    name = "Actual FMV",
    labels = scales::label_dollar(
      accuracy = 1,
      scale = 1 / 1000,
      suffix = "K"
    ),
    n.breaks = 5,
    limits = c(1e4, 1.5e6)
  ) +
  theme_minimal()

ggplotly(pred_v_actual_class)
```

:::

## 211/212s

::: {.panel-tabset}

## YoY Changes

Where are 211s and 212s increasing a *lot*? Near values are 2022 Certified values, far values are 2020 Board values.

``` {r}
assessment_pin %>%
  filter(
    meta_class %in% c("211", "212"),
    meta_triad_code == run_triad_code
  ) %>%
  mutate(
    yoy_far = abs((pred_pin_final_fmv - prior_far_tot) / pred_pin_final_fmv),
    yoy_near = abs((pred_pin_final_fmv - prior_near_tot) / pred_pin_final_fmv)
  ) %>%
  mutate(
    big_swing_near = yoy_near > 0.5,
    big_swing_far = yoy_far > 0.5
  ) %>%
  group_by(meta_township_code, meta_class) %>%
  summarise(
    big_swings_near = sum(as.numeric(big_swing_near), na.rm = TRUE) / n(),
    big_swings_far = sum(as.numeric(big_swing_far), na.rm = TRUE) / n()
  ) %>%
  ungroup() %>%
  arrange(desc(big_swings_near)) %>%
  mutate(across(starts_with("big"), ~ label_percent(accuracy = 0.1)(.x))) %>%
  left_join(
    ccao::town_dict %>% select(meta_township_code = township_code, township_name)
  ) %>%
  select(
    "Township Name" = township_name,
    Class = meta_class,
    "% Delta Near > 50%" = big_swings_near,
    "% Delta Far > 50%" = big_swings_far
  ) %>%
  datatable(
    rownames = FALSE,
    height = "500px",
    options = list(
      columnDefs = list(
        list(className = "dt-center", targets = c(1:2))
      )
    )
  )
```

## Large Sqft. 212s

``` {r}
training_data %>%
  filter(
    meta_class %in% c("211", "212"),
    meta_triad_code == run_triad_code,
    !ind_pin_is_multicard
  ) %>%
  group_by(meta_class) %>%
  mutate(
    mean_sf = round(mean(char_bldg_sf, na.rm = TRUE), 0),
    outlier = abs(char_bldg_sf) >
      mean(char_bldg_sf, na.rm = TRUE) + 3 * sd(char_bldg_sf, na.rm = TRUE)
  ) %>%
  ungroup() %>%
  filter(outlier) %>%
  select(
    PIN = meta_pin,
    Township = meta_township_name,
    Class = meta_class,
    "Mean SF" = mean_sf,
    "Building SF" = char_bldg_sf
  ) %>%
  datatable(
    rownames = FALSE,
    height = "500px",
    options = list(
      columnDefs = list(
        list(className = "dt-center", targets = c(1:4))
      )
    )
  )
```

:::

## 210s/295s

How far apart are the initial predicted values for townhomes? Is the average they get assigned reasonable?

``` {r}
assessment_pin %>%
  filter(!is.na(meta_complex_id) & meta_triad_code == run_triad_code) %>%
  group_by(meta_complex_id) %>%
  mutate(
    fmvpsf = pred_pin_final_fmv / char_total_bldg_sf
  ) %>%
  summarise(
    n = n(),
    "Min Initial FMV" = min(pred_pin_initial_fmv, na.rm = TRUE),
    "Max Initial FMV" = max(pred_pin_initial_fmv, na.rm = TRUE),
    "Initial % Range" = (`Max Initial FMV` - `Min Initial FMV`) / `Min Initial FMV`,
    "Mean Final FMV" = mean(pred_pin_final_fmv, na.rm = TRUE),
    "FMV per SQFT SD" = sd(fmvpsf, na.rm = TRUE)
  ) %>%
  slice_max(order_by = `Initial % Range`, n = 20) %>%
  mutate(
    across(contains("SQFT"), ~ round(.x, 1)),
    across(contains("FMV"), dollar),
    across(contains("%"), ~ percent(.x, accuracy = 0.1))
  ) %>%
  relocate(c(meta_complex_id, n)) %>%
  rename("Complex ID" = meta_complex_id, "Count" = n) %>%
  datatable(
    rownames = FALSE,
    height = "500px",
    options = list(
      columnDefs = list(
        list(className = "dt-center", targets = c(1:6))
      )
    )
  )
```

## Data Quality Checks

::: {.panel-tabset}

## HIE Spot Check

Are HIE characteristics getting accurately updated?

```{r}
AWS_ATHENA_CONN_NOCTUA <- DBI::dbConnect(noctua::athena())

hie_pins <- training_data %>%
  filter(
    hie_num_active == 1,
    meta_triad_code == run_triad_code,
    meta_year == as.numeric(params$year) - 1
  ) %>%
  pull(meta_pin) %>%
  unique()

hie_data <- DBI::dbGetQuery(
  conn = AWS_ATHENA_CONN_NOCTUA, glue::glue_sql("
  SELECT *
  FROM ccao.hie
  WHERE pin IN ({hie_pins*})
  AND qu_sqft_bld > 0
  AND year >= '2018'
  ",
    .con = AWS_ATHENA_CONN_NOCTUA
  )
)

chars_data <- DBI::dbGetQuery(
  conn = AWS_ATHENA_CONN_NOCTUA, glue::glue_sql("
  SELECT *
  FROM default.vw_card_res_char
  WHERE pin IN ({hie_pins*})
  AND year = '{as.numeric(params$year) - 1}'
  ",
    .con = AWS_ATHENA_CONN_NOCTUA
  )
)

chars_data %>%
  inner_join(hie_data %>% select(-year), by = c("pin")) %>%
  left_join(
    training_data %>% select(
      meta_pin, meta_year,
      train_sqft = char_bldg_sf,
      train_fbath = char_fbath, train_air = char_air
    ),
    by = c("pin" = "meta_pin", "year" = "meta_year")
  ) %>%
  select(
    pin,
    `OG SQFT` = char_bldg_sf,
    `HIE SQFT` = qu_sqft_bld,
    `NEW SQFT` = train_sqft,
    `OG FBATH` = char_fbath,
    `HIE FBATH` = qu_full_bath,
    `NEW FBATH` = train_fbath,
    `OG AIR` = char_air,
    `HIE AIR` = qu_air,
    `NEW AIR` = train_air
  ) %>%
  datatable(rownames = FALSE)
```

## Characteristic Filling

Is time filling working as expected?

```{r}
training_data %>%
  group_by(meta_pin) %>%
  filter(
    !ind_pin_is_multicard,
    min(meta_sale_date) < lubridate::make_date(2019),
    max(meta_sale_date) > lubridate::make_date(2019),
  ) %>%
  filter(n() > 6) %>%
  ungroup() %>%
  select(
    PIN = meta_pin,
    Year = meta_year,
    `Sale Date` = meta_sale_date,
    `FS Flood Factor (2019)` = loc_env_flood_fs_factor,
    `Walkability (2017)` = loc_access_cmap_walk_total_score,
    `GS Rating (2021)` = prox_avg_school_rating_in_half_mile
  ) %>%
  arrange(PIN, Year) %>%
  datatable(rownames = FALSE)
```

## Multi-Card and Multi-PIN Aggregation

```{r}
test_card_pins <- c(
  "17321110470000", "05174150240000",
  "05213220250000", "08121220400000", "06334030310000"
)

assessment_card %>%
  filter(meta_pin %in% test_card_pins) %>%
  arrange(meta_pin, meta_card_num) %>%
  select(
    PIN = meta_pin, Class = meta_class, Card = meta_card_num,
    `Card %` = meta_card_pct_total_fmv, SQFT = char_bldg_sf,
    `Card Initial` = pred_card_initial_fmv,
    `Card Final` = pred_card_final_fmv
  ) %>%
  left_join(
    assessment_pin %>%
      select(
        PIN = meta_pin, Proration = meta_tieback_proration_rate,
        `PIN Far` = prior_far_tot,
        `PIN Near` = prior_near_tot,
        `PIN Initial` = pred_pin_initial_fmv,
        `PIN Final` = pred_pin_final_fmv_round
      ),
    by = c("PIN")
  ) %>%
  datatable(rownames = FALSE)
```

## Land Valuation

```{r, out.width="100%"}
assessment_pin %>%
  filter(meta_triad_code == run_triad_code) %>%
  mutate(land_pct = pred_pin_final_fmv_land / pred_pin_final_fmv_round) %>%
  select(
    meta_pin, land_pct,
    pred_pin_final_fmv_bldg, pred_pin_final_fmv_land,
    pred_pin_final_fmv_round
  ) %>%
  ggplot() +
  geom_histogram(aes(x = land_pct)) +
  scale_x_continuous(
    name = "Land % of Total FMV",
    labels = scales::label_percent()
  ) +
  labs(y = "Count") +
  theme_minimal()
```

### PINs With 0 Land Sqft.

```{r}
assessment_pin %>%
  filter(meta_triad_code == run_triad_code) %>%
  filter(pred_pin_final_fmv_land == 0) %>%
  datatable(rownames = FALSE)
```

:::
